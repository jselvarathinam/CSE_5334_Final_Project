{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7cJPd0w3feaLX/GIQL0Ia",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jselvarathinam/CSE_5334_Final_Project/blob/main/CSE5334_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Packages and Data"
      ],
      "metadata": {
        "id": "Ok0ago1XhTz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SLda9V-hYOv",
        "outputId": "59dac6d5-66cf-459b-bd71-6eab2fafcad4"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l52ubZdvW8H",
        "outputId": "0c9a7717-70b2-427b-fe81-bd14bb726d58"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.9.24)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing Necessary packages\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile\n",
        "import zipfile\n",
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, shutil, math, cv2, json, random\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.layers import *\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from PIL import Image\n",
        "from numpy import asarray\n",
        "from matplotlib.image import imread\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "TALYa0GWKS2S"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The following four cells make a kaggle directory in this workspace and download the caltech101 dataset into it.\n",
        "I learned how to do this from the following youtube tutorial: https://www.youtube.com/watch?v=yEXkEUqK52Q. I think \n",
        "this code is fairly common but I wanted to include a reference just in case.\n",
        "\"\"\"\n",
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPmeprzZtycX",
        "outputId": "d1ff7cfa-fa75-4c33-c18b-3210c5fc601b"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp /content/drive/MyDrive/ColabNotebooks/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "0iLlaWW1tzrP"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "FKegvfORuzi9"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d whenamancodes/wild-animals-images #Need to download only specific folders\n",
        "! mkdir wild-animals-images\n",
        "! mv wild-animals-images.zip wild-animals-images\n",
        "%cd wild-animals-images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY3S7kyRuzsH",
        "outputId": "e7daecf2-8aee-4a35-9d85-081d4efca508"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading wild-animals-images.zip to /content/wild-animals-images/wild-animals-images\n",
            "100% 0.98G/0.98G [00:05<00:00, 169MB/s]\n",
            "100% 0.98G/0.98G [00:05<00:00, 184MB/s]\n",
            "mkdir: cannot create directory ‘wild-animals-images’: File exists\n",
            "/content/wild-animals-images/wild-animals-images/wild-animals-images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip wild-animals-images.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtgQTqOru5zX",
        "outputId": "5ec7ad51-13ca-416e-ee64-af6211067dd4"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  wild-animals-images.zip\n",
            "  inflating: cheetah-resize-224/resize-224/00000000_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000001_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000002_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000003_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000004_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000005_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000007_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000008_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000009_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000010_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000011_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000012_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000014_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000015_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000017_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000018_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000019_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000020_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000021_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000022_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000023_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000025_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000027_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000028_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000029_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000030_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000031_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000032_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000033_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000034_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000035_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000036_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000037_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000038_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000039_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000040_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000041_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000042_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000043_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000044_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000049_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000050_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000051_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000052_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000054_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000055_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000056_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000057_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000058_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000059_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000060_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000061_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000062_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000063_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000065_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000066_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000067_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000068_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000070_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000072_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000073_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000077_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000078_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000079_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000080_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000081_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000083_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000084_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000085_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000086_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000087_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000088_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000089_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000091_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000092_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000094_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000095_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000096_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000097_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000098_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000101_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000102_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000103_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000104_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000108_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000109_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000111_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000113_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000115_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000116_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000117_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000118_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000119_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000120_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000122_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000124_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000125_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000126_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000127_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000129_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000131_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000132_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000134_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000136_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000137_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000138_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000139_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000145_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000146_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000147_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000148_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000150_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000151_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000152_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000154_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000155_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000159_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000160_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000161_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000162_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000163_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000165_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000166_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000167_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000168_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000169_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000170_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000172_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000173_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000174_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000175_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000177_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000178_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000179_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000180_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000182_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000183_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000185_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000186_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000187_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000189_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000191_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000192_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000195_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000197_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000198_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000199_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000200_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000201_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000202_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000203_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000204_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000205_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000206_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000207_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000208_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000209_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000210_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000211_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000214_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000215_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000216_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000217_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000219_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000220_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000221_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000222_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000224_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000225_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000226_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000227_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000230_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000232_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000234_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000235_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000236_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000239_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000240_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000242_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000244_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000245_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000246_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000247_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000248_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000250_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000251_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000252_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000254_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000255_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000256_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000257_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000258_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000262_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000263_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000264_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000265_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000266_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000267_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000268_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000271_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000272_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000273_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000274_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000276_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000277_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000278_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000279_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000281_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000283_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000285_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000288_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000289_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000291_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000292_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000294_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000295_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000296_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000299_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000301_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000304_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000305_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000306_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000307_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000308_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000309_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000310_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000311_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000312_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000313_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000315_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000316_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000317_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000318_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000319_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000325_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000326_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000328_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000329_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000330_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000333_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000338_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000339_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000340_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000341_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000344_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000345_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000348_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000351_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000353_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000354_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000355_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000356_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000357_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000359_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000360_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000362_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000368_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000369_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000371_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000372_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000373_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000375_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000379_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000380_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000381_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000382_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000383_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000384_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000386_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000387_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000388_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000390_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000391_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000392_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000393_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000395_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000397_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000398_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000399_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000401_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000402_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000404_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000406_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000409_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000413_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000414_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000416_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000417_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000418_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000421_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000423_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000424_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000433_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000434_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000436_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000440_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000445_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000446_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000448_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000449_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000453_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000456_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000458_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000460_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000461_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000463_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000468_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000469_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000471_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000472_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000473_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000483_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000484_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000485_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000489_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000491_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000500_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000501_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000506_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000509_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000510_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000521_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000523_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000527_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000532_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000533_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000534_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000538_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000541_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000547_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000549_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000562_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000563_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000564_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000567_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000568_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000569_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000571_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000580_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000587_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000593_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000595_224resized.png  \n",
            "  inflating: cheetah-resize-224/resize-224/00000596_224resized.png  \n",
            "replace cheetah-resize-300/resize-300/00000000_300resized.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -r cheetah-resize-224\n",
        "! rm -r cheetah-resize-512\n",
        "! rm -r fox-resize-224\n",
        "! rm -r fox-resize-512\n",
        "! rm -r hyena-resize-224\n",
        "! rm -r hyena-resize-512\n",
        "! rm -r lion-resize-224\n",
        "! rm -r lion-resize-512\n",
        "! rm -r tiger-resize-224\n",
        "! rm -r tiger-resize-512\n",
        "! rm -r wolf-resize-224\n",
        "! rm -r wolf-resize-512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AliXdKY4NXdf",
        "outputId": "eacc5883-217b-4099-f2eb-594385407d1e"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'cheetah-resize-512': No such file or directory\n",
            "rm: cannot remove 'fox-resize-224': No such file or directory\n",
            "rm: cannot remove 'fox-resize-512': No such file or directory\n",
            "rm: cannot remove 'hyena-resize-224': No such file or directory\n",
            "rm: cannot remove 'hyena-resize-512': No such file or directory\n",
            "rm: cannot remove 'lion-resize-224': No such file or directory\n",
            "rm: cannot remove 'lion-resize-512': No such file or directory\n",
            "rm: cannot remove 'tiger-resize-224': No such file or directory\n",
            "rm: cannot remove 'tiger-resize-512': No such file or directory\n",
            "rm: cannot remove 'wolf-resize-224': No such file or directory\n",
            "rm: cannot remove 'wolf-resize-512': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/wild-animals-images/wild-animals-images\n",
        "! mkdir cheetah\n",
        "! mkdir fox\n",
        "! mkdir hyena\n",
        "! mkdir lion\n",
        "! mkdir tiger\n",
        "! mkdir wolf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B21oxUwZRrB4",
        "outputId": "2197a638-a1e3-4589-b8e1-bbb53e1332d2"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/wild-animals-images/wild-animals-images\n",
            "mkdir: cannot create directory ‘cheetah’: File exists\n",
            "mkdir: cannot create directory ‘fox’: File exists\n",
            "mkdir: cannot create directory ‘hyena’: File exists\n",
            "mkdir: cannot create directory ‘lion’: File exists\n",
            "mkdir: cannot create directory ‘tiger’: File exists\n",
            "mkdir: cannot create directory ‘wolf’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mv /content/wild-animals-images/cheetah-resize-300/resize-300/* /content/wild-animals-images/cheetah\n",
        "! mv /content/wild-animals-images/fox-resize-300/fox-resize-300/* /content/wild-animals-images/fox\n",
        "! mv /content/wild-animals-images/hyena-resize-300/resize-300/* /content/wild-animals-images/hyena\n",
        "! mv /content/wild-animals-images/lion-resize-300/lion-resize-300/* /content/wild-animals-images/lion\n",
        "! mv /content/wild-animals-images/tiger-resize-300/tiger-resize-300/* /content/wild-animals-images/tiger\n",
        "! mv /content/wild-animals-images/wolf-resize-300/wolf-resize-300/* /content/wild-animals-images/wolf\n",
        "\n",
        "! rm -r /content/wild-animals-images/cheetah-resize-300\n",
        "! rm -r /content/wild-animals-images/fox-resize-300\n",
        "! rm -r /content/wild-animals-images/hyena-resize-300\n",
        "! rm -r /content/wild-animals-images/lion-resize-300\n",
        "! rm -r /content/wild-animals-images/tiger-resize-300\n",
        "! rm -r /content/wild-animals-images/wolf-resize-300\n",
        "! rm -r /content/wild-animals-images/wild-animals-images.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJM1BGdSSXU-",
        "outputId": "1c30a433-3280-40a1-9f3c-695f16d3f86d"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/content/wild-animals-images/cheetah-resize-300/resize-300/*': No such file or directory\n",
            "mv: cannot stat '/content/wild-animals-images/fox-resize-300/fox-resize-300/*': No such file or directory\n",
            "mv: cannot stat '/content/wild-animals-images/hyena-resize-300/resize-300/*': No such file or directory\n",
            "mv: cannot stat '/content/wild-animals-images/lion-resize-300/lion-resize-300/*': No such file or directory\n",
            "mv: cannot stat '/content/wild-animals-images/tiger-resize-300/tiger-resize-300/*': No such file or directory\n",
            "mv: cannot stat '/content/wild-animals-images/wolf-resize-300/wolf-resize-300/*': No such file or directory\n",
            "rm: cannot remove '/content/wild-animals-images/cheetah-resize-300': No such file or directory\n",
            "rm: cannot remove '/content/wild-animals-images/fox-resize-300': No such file or directory\n",
            "rm: cannot remove '/content/wild-animals-images/hyena-resize-300': No such file or directory\n",
            "rm: cannot remove '/content/wild-animals-images/lion-resize-300': No such file or directory\n",
            "rm: cannot remove '/content/wild-animals-images/tiger-resize-300': No such file or directory\n",
            "rm: cannot remove '/content/wild-animals-images/wolf-resize-300': No such file or directory\n",
            "rm: cannot remove '/content/wild-animals-images/wild-animals-images.zip': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Training, Testing, Validation Data on 80/10/10 split\n"
      ],
      "metadata": {
        "id": "O3H2R6a_hfwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This code in this cell is from https://www.kaggle.com/code/jiaowoguanren/airplanes-motorbikes-schooners-tf-efficientnet \n",
        "I used it to create a train/test/validate split for my data. (It also helped me in the next cell to understand I needed to use the \n",
        "\".flow_from_dataframe\" when generating batches of images and labels for training, testing and validating the neural net). \n",
        "'''\n",
        "sdir = '/content/wild-animals-images'\n",
        "\n",
        "#Defining function that creates training, testing, and validation dataframes\n",
        "def make_dataframes(sdir):\n",
        "    bad_images = []\n",
        "    good_ext = ['jpg', 'jpeg', 'png', 'tiff']\n",
        "    filepaths = []\n",
        "    labels = []\n",
        "    classes = sorted(os.listdir(sdir))\n",
        "    for klass in classes:\n",
        "        classpath = os.path.join(sdir, klass)\n",
        "        flist = sorted(os.listdir(classpath))\n",
        "        desc = f'{klass:23s}'\n",
        "        for f in tqdm(flist, ncols=110, desc=desc, unit='file', colour='blue'):\n",
        "            fpath = os.path.join(classpath, f)\n",
        "            fl = f.lower()\n",
        "            index = fl.rfind('.')\n",
        "            ext = fl[index + 1:]\n",
        "            if ext in good_ext:\n",
        "                try:\n",
        "                    img = cv2.imread(fpath)\n",
        "                    shape = img.shape\n",
        "                    filepaths.append(fpath)\n",
        "                    labels.append(klass)\n",
        "                except:\n",
        "                    bad_images.append(fpath)\n",
        "                    print('defective image file: ', fpath)\n",
        "            else:\n",
        "                bad_images.append(fpath)\n",
        "    Fseries = pd.Series(filepaths, name='filepaths')\n",
        "    Lseries = pd.Series(labels, name='labels')\n",
        "    df = pd.concat([Fseries, Lseries], axis=1)\n",
        "\n",
        "    train_df, dummy_df = train_test_split(df, train_size=.8, shuffle=True, random_state=123, stratify=df['labels'])\n",
        "    valid_df, test_df = train_test_split(dummy_df, train_size=.5, shuffle=True, random_state=123,\n",
        "                                         stratify=dummy_df['labels'])\n",
        "    classes = sorted(train_df['labels'].unique())\n",
        "    class_count = len(classes)\n",
        "    sample_df = train_df.sample(n=50, replace=False)\n",
        "\n",
        "    ht = 0\n",
        "    wt = 0\n",
        "    count = 0\n",
        "    \n",
        "    for i in range(len(sample_df)):\n",
        "        fpath = sample_df['filepaths'].iloc[i]\n",
        "        try:\n",
        "            img = cv2.imread(fpath)\n",
        "            h = img.shape[0]\n",
        "            w = img.shape[1]\n",
        "            wt += w\n",
        "            ht += h\n",
        "            count += 1\n",
        "        except:\n",
        "            pass\n",
        "    have = int(ht / count)\n",
        "    wave = int(wt / count)\n",
        "    aspect_ratio = have / wave\n",
        "    print('number of classes in processed dataset= ', class_count)\n",
        "    counts = list(train_df['labels'].value_counts())\n",
        "    print('the maximum files in any class in train_df is ', max(counts),\n",
        "          '  the minimum files in any class in train_df is ', min(counts))\n",
        "    print('train_df length: ', len(train_df), '  test_df length: ', len(test_df), '  valid_df length: ', len(valid_df))\n",
        "    print('average image height= ', have, '  average image width= ', wave, ' aspect ratio h/w= ', aspect_ratio)\n",
        "    return train_df, test_df, valid_df, classes, class_count\n",
        "\n",
        "#Create training, testing, and validation dataframes\n",
        "train_df, test_df, valid_df, classes, class_count = make_dataframes(sdir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj1OvW-ohh1f",
        "outputId": "c9610b4d-26d0-41c1-c493-aef297da43fd"
      },
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "cheetah                :  62%|\u001b[34m██████████████████████████▋                \u001b[0m| 213/343 [00:00<00:00, 223.52file/s]\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defective image file:  /content/wild-animals-images/cheetah/00000244_300resized.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "cheetah                : 100%|\u001b[34m███████████████████████████████████████████\u001b[0m| 343/343 [00:01<00:00, 221.98file/s]\u001b[0m\n",
            "fox                    : 100%|\u001b[34m███████████████████████████████████████████\u001b[0m| 250/250 [00:01<00:00, 222.63file/s]\u001b[0m\n",
            "hyena                  :  63%|\u001b[34m██████████████████████████▉                \u001b[0m| 192/306 [00:00<00:00, 231.61file/s]\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defective image file:  /content/wild-animals-images/hyena/00000224_300resized.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "hyena                  : 100%|\u001b[34m███████████████████████████████████████████\u001b[0m| 306/306 [00:01<00:00, 231.04file/s]\u001b[0m\n",
            "lion                   : 100%|\u001b[34m███████████████████████████████████████████\u001b[0m| 294/294 [00:01<00:00, 242.66file/s]\u001b[0m\n",
            "tiger                  : 100%|\u001b[34m███████████████████████████████████████████\u001b[0m| 269/269 [00:01<00:00, 214.73file/s]\u001b[0m\n",
            "wild-animals-images    : 100%|\u001b[34m███████████████████████████████████████████\u001b[0m| 13/13 [00:00<00:00, 76047.35file/s]\u001b[0m\n",
            "wolf                   : 100%|\u001b[34m███████████████████████████████████████████\u001b[0m| 263/263 [00:01<00:00, 220.28file/s]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of classes in processed dataset=  6\n",
            "the maximum files in any class in train_df is  274   the minimum files in any class in train_df is  200\n",
            "train_df length:  1378   test_df length:  173   valid_df length:  172\n",
            "average image height=  300   average image width=  300  aspect ratio h/w=  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network Run 1"
      ],
      "metadata": {
        "id": "w1Hp1xl1vUJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The code in this cell and the next 2 cells is loosely based on the code in the following tutorial: \n",
        "https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/Course%202%20-%20Part%208%20-%20Lesson%202%20-%20Notebook%20(RockPaperScissors).ipynb#scrollTo=ZABJp7T3VLCU\n",
        "I say loosely based because while I learned the essence of how to set up, compile and fit the model from this tutorial it would not have worked for the problem I am trying to solve,\n",
        "I had to learn what all the code, parameters and hyperparamters meant from documentation and tutorials (that I've linked in my blog post) and then write code that would actually work for \n",
        "my particular problem. Therefore these next few cells are where most of my unique contributions are.\n",
        "\"\"\"\n",
        "#Rescaling images\n",
        "train_datagen = ImageDataGenerator(rescale= 1.0/255)\n",
        "test_datagen = ImageDataGenerator(rescale= 1.0/255)\n",
        "valid_datagen = ImageDataGenerator(rescale= 1.0/255) \n",
        "\n",
        "#Generating batches of images for training, validation, and testing along with their labels to be fed into the neural net\n",
        "#Read documentation on .flow_from_dataframe() from tensorflow website: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_dataframe\n",
        "train_data = train_datagen.flow_from_dataframe(train_df, x_col = \"filepaths\", y_col = \"labels\", batch_size =50, class_mode= \"categorical\", target_size = (224,224))\n",
        "test_data = test_datagen.flow_from_dataframe(test_df, x_col = \"filepaths\", y_col = \"labels\", batch_size =50, class_mode= \"categorical\", target_size = (224,224))\n",
        "valid_data = valid_datagen.flow_from_dataframe(valid_df, x_col = \"filepaths\", y_col = \"labels\", batch_size =50, class_mode= \"categorical\", target_size = (224,224))"
      ],
      "metadata": {
        "id": "MwWR3t-0zsNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cfa9381-e6b5-4003-8c31-c13c2e92dbc5"
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1378 validated image filenames belonging to 6 classes.\n",
            "Found 173 validated image filenames belonging to 6 classes.\n",
            "Found 172 validated image filenames belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    \n",
        "    #First convolution Layer\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation= \"relu\", input_shape= (224,224,3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Second convolution layer\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation= \"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Flattening to feed into DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "\n",
        "    #Hidden Layer with 512 neurons\n",
        "    tf.keras.layers.Dense(512, activation= 'relu'),\n",
        "\n",
        "    #Output layer\n",
        "    tf.keras.layers.Dense(6, activation= 'softmax')\n",
        "\n",
        "    ])\n",
        "\n",
        "model.compile(loss= \"mean_squared_error\", optimizer= RMSprop(lr=0.001), metrics = [\"acc\"])\n"
      ],
      "metadata": {
        "id": "90bvn-fOqUST"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fitting the model\n",
        "history = model.fit(train_data, \n",
        "                    epochs = 20, \n",
        "                    steps_per_epoch = 5, \n",
        "                    validation_data = valid_data, \n",
        "                    validation_steps = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D62ObHOKqWMZ",
        "outputId": "55f57da1-dd93-433b-f415-c825507425d4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1/5 [=====>........................] - ETA: 17s - loss: 0.1374 - acc: 0.2800"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - ETA: 0s - loss: 0.2634 - acc: 0.1272"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 5 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5/5 [==============================] - 19s 4s/step - loss: 0.2634 - acc: 0.1272 - val_loss: 0.2810 - val_acc: 0.1570\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - 13s 3s/step - loss: 0.2853 - acc: 0.1440\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - 13s 2s/step - loss: 0.2893 - acc: 0.1320\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - 13s 2s/step - loss: 0.2787 - acc: 0.1640\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.2895 - acc: 0.1316\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.2865 - acc: 0.1404\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.2907 - acc: 0.1280\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - 13s 3s/step - loss: 0.2840 - acc: 0.1480\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - 13s 3s/step - loss: 0.2880 - acc: 0.1360\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.2867 - acc: 0.1400\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - 13s 2s/step - loss: 0.2880 - acc: 0.1360\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.2734 - acc: 0.1798\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.2822 - acc: 0.1535\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - 13s 2s/step - loss: 0.2933 - acc: 0.1200\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - 13s 3s/step - loss: 0.2947 - acc: 0.1160\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - 13s 3s/step - loss: 0.2800 - acc: 0.1600\n",
            "Epoch 17/20\n",
            "5/5 [==============================] - 14s 3s/step - loss: 0.2787 - acc: 0.1640\n",
            "Epoch 18/20\n",
            "5/5 [==============================] - 13s 3s/step - loss: 0.2880 - acc: 0.1360\n",
            "Epoch 19/20\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.2851 - acc: 0.1447\n",
            "Epoch 20/20\n",
            "5/5 [==============================] - 16s 3s/step - loss: 0.2840 - acc: 0.1480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network Run 2"
      ],
      "metadata": {
        "id": "--ZIrBbRy2hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    \n",
        "    #First convolution Layer\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation= \"relu\", input_shape= (224,224,3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Second convolution layer\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation= \"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Third convolution layer\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation= \"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Fourth convolution layer\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation= \"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Flattening to feed into DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "\n",
        "    #Hidden Layer with 512 neurons\n",
        "    tf.keras.layers.Dense(512, activation= 'relu'),\n",
        "\n",
        "    #Output layer\n",
        "    tf.keras.layers.Dense(6, activation= 'softmax')\n",
        "\n",
        "    ])\n",
        "\n",
        "model.compile(loss= \"mean_squared_error\", optimizer= RMSprop(lr=0.001), metrics = [\"acc\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcVmYSVAwhQf",
        "outputId": "4570ddaa-446a-4f7b-aabe-230c39d16e74"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fitting the model\n",
        "history = model.fit(train_data, \n",
        "                    epochs = 20, \n",
        "                    steps_per_epoch = 5, \n",
        "                    validation_data = valid_data, \n",
        "                    validation_steps = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SN3QYR5wh2W",
        "outputId": "6ea48dd8-4e18-4d03-e58a-b0d27df49221"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "4/5 [=======================>......] - ETA: 2s - loss: 0.1926 - acc: 0.1850"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5/5 [==============================] - ETA: 0s - loss: 0.1818 - acc: 0.1800"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 5 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5/5 [==============================] - 22s 4s/step - loss: 0.1818 - acc: 0.1800 - val_loss: 0.1372 - val_acc: 0.2791\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - 13s 3s/step - loss: 0.1384 - acc: 0.2280\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - 13s 3s/step - loss: 0.1386 - acc: 0.1480\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - 13s 3s/step - loss: 0.1418 - acc: 0.1680\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - 14s 3s/step - loss: 0.1365 - acc: 0.2680\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.1346 - acc: 0.2763\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - 14s 3s/step - loss: 0.1348 - acc: 0.2680\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - 13s 3s/step - loss: 0.1322 - acc: 0.2520\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - 16s 3s/step - loss: 0.1290 - acc: 0.3280\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - 14s 3s/step - loss: 0.1247 - acc: 0.3400\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - 14s 3s/step - loss: 0.1187 - acc: 0.3960\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - 16s 3s/step - loss: 0.1408 - acc: 0.3400\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - 14s 3s/step - loss: 0.1136 - acc: 0.4800\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - 14s 3s/step - loss: 0.1173 - acc: 0.4240\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - 14s 3s/step - loss: 0.1142 - acc: 0.4600\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - 13s 3s/step - loss: 0.1123 - acc: 0.4440\n",
            "Epoch 17/20\n",
            "5/5 [==============================] - 14s 3s/step - loss: 0.1172 - acc: 0.4400\n",
            "Epoch 18/20\n",
            "5/5 [==============================] - 15s 3s/step - loss: 0.1164 - acc: 0.4560\n",
            "Epoch 19/20\n",
            "5/5 [==============================] - 19s 4s/step - loss: 0.0973 - acc: 0.5760\n",
            "Epoch 20/20\n",
            "5/5 [==============================] - 12s 2s/step - loss: 0.1132 - acc: 0.4605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network Run 3"
      ],
      "metadata": {
        "id": "Vgok4gBuzB5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    \n",
        "    #First convolution Layer\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation= \"relu\", input_shape= (224,224,3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Second convolution layer\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation= \"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Third convolution layer\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation= \"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Fourth convolution layer\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation= \"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Flattening to feed into DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "\n",
        "    #Hidden Layer with 512 neurons\n",
        "    tf.keras.layers.Dense(512, activation= 'relu'),\n",
        "\n",
        "    #Output layer\n",
        "    tf.keras.layers.Dense(6, activation= 'softmax')\n",
        "\n",
        "    ])\n",
        "\n",
        "model.compile(loss= \"mean_squared_error\", optimizer= RMSprop(lr=0.001), metrics = [\"acc\"])"
      ],
      "metadata": {
        "id": "iIR7EvDWzCD7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fitting the model\n",
        "history = model.fit(train_data, \n",
        "                    epochs = 20, \n",
        "                    steps_per_epoch = 10, \n",
        "                    validation_data = valid_data, \n",
        "                    validation_steps = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWoWnssqzCIi",
        "outputId": "da47f998-9e49-45e9-9d17-9bea2323ce8d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.2556 - acc: 0.1440"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 5 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10/10 [==============================] - 34s 3s/step - loss: 0.2556 - acc: 0.1440 - val_loss: 0.1636 - val_acc: 0.1570\n",
            "Epoch 2/20\n",
            "10/10 [==============================] - 27s 3s/step - loss: 0.1411 - acc: 0.1700\n",
            "Epoch 3/20\n",
            "10/10 [==============================] - 26s 3s/step - loss: 0.1383 - acc: 0.1820\n",
            "Epoch 4/20\n",
            "10/10 [==============================] - 29s 3s/step - loss: 0.1396 - acc: 0.2200\n",
            "Epoch 5/20\n",
            "10/10 [==============================] - 27s 3s/step - loss: 0.1376 - acc: 0.2620\n",
            "Epoch 6/20\n",
            "10/10 [==============================] - 27s 3s/step - loss: 0.1345 - acc: 0.2640\n",
            "Epoch 7/20\n",
            "10/10 [==============================] - 29s 3s/step - loss: 0.1364 - acc: 0.3080\n",
            "Epoch 8/20\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.1349 - acc: 0.3682\n",
            "Epoch 9/20\n",
            "10/10 [==============================] - 26s 3s/step - loss: 0.1231 - acc: 0.3900\n",
            "Epoch 10/20\n",
            "10/10 [==============================] - 25s 3s/step - loss: 0.1194 - acc: 0.4017\n",
            "Epoch 11/20\n",
            "10/10 [==============================] - 27s 3s/step - loss: 0.1163 - acc: 0.4620\n",
            "Epoch 12/20\n",
            "10/10 [==============================] - 27s 3s/step - loss: 0.1117 - acc: 0.4800\n",
            "Epoch 13/20\n",
            "10/10 [==============================] - 27s 3s/step - loss: 0.1155 - acc: 0.4340\n",
            "Epoch 14/20\n",
            "10/10 [==============================] - 28s 3s/step - loss: 0.1033 - acc: 0.5146\n",
            "Epoch 15/20\n",
            "10/10 [==============================] - 26s 3s/step - loss: 0.1061 - acc: 0.5280\n",
            "Epoch 16/20\n",
            "10/10 [==============================] - 26s 3s/step - loss: 0.0964 - acc: 0.5439\n",
            "Epoch 17/20\n",
            "10/10 [==============================] - 26s 3s/step - loss: 0.0984 - acc: 0.5397\n",
            "Epoch 18/20\n",
            "10/10 [==============================] - 30s 3s/step - loss: 0.0911 - acc: 0.5962\n",
            "Epoch 19/20\n",
            "10/10 [==============================] - 26s 3s/step - loss: 0.0941 - acc: 0.5860\n",
            "Epoch 20/20\n",
            "10/10 [==============================] - 28s 3s/step - loss: 0.0790 - acc: 0.6402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network Run 4"
      ],
      "metadata": {
        "id": "K9wZDhvHhza4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    \n",
        "    #First convolution Layer\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation= \"relu\", input_shape= (224,224,3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Second convolution layer\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation= \"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Third convolution layer\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation= \"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Fourth convolution layer\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation= \"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Flattening to feed into DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "\n",
        "    #Hidden Layer with 512 neurons\n",
        "    tf.keras.layers.Dense(512, activation= 'relu'),\n",
        "\n",
        "    #Output layer\n",
        "    tf.keras.layers.Dense(6, activation= 'softmax')\n",
        "\n",
        "    ])\n",
        "\n",
        "model.compile(loss= \"mean_squared_error\", optimizer= RMSprop(lr=0.001), metrics = [\"acc\"])"
      ],
      "metadata": {
        "id": "ddLmTZkk8BUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fitting the model\n",
        "history = model.fit(train_data, \n",
        "                    epochs = 20, \n",
        "                    steps_per_epoch = 20, \n",
        "                    validation_data = valid_data, \n",
        "                    validation_steps = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuNWPeEq2fXL",
        "outputId": "b35b7105-bfa8-4c30-d9a6-8a6b326c394d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.2674 - acc: 0.1677"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 5 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r20/20 [==============================] - 59s 3s/step - loss: 0.2674 - acc: 0.1677 - val_loss: 0.2771 - val_acc: 0.1686\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 53s 3s/step - loss: 0.1901 - acc: 0.1769\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 54s 3s/step - loss: 0.1389 - acc: 0.2220\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 54s 3s/step - loss: 0.1347 - acc: 0.2556\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 52s 3s/step - loss: 0.1305 - acc: 0.3630\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 54s 3s/step - loss: 0.1210 - acc: 0.4059\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 51s 3s/step - loss: 0.1107 - acc: 0.4857\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 54s 3s/step - loss: 0.1086 - acc: 0.4650\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 53s 3s/step - loss: 0.1019 - acc: 0.5204\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 53s 3s/step - loss: 0.0945 - acc: 0.5750\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 54s 3s/step - loss: 0.0786 - acc: 0.6620\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 54s 3s/step - loss: 0.0713 - acc: 0.6871\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 52s 3s/step - loss: 0.0628 - acc: 0.7413\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 54s 3s/step - loss: 0.0597 - acc: 0.7444\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 53s 3s/step - loss: 0.0375 - acc: 0.8466\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 53s 3s/step - loss: 0.0392 - acc: 0.8460\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 54s 3s/step - loss: 0.0256 - acc: 0.9018\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 54s 3s/step - loss: 0.0234 - acc: 0.9172\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 52s 3s/step - loss: 0.0222 - acc: 0.9080\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 54s 3s/step - loss: 0.0145 - acc: 0.9479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    \n",
        "    #First convolution Layer\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation= \"relu\", input_shape= (224,224,3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Second convolution layer\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation= \"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Third convolution layer\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation= \"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Fourth convolution layer\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation= \"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    #Flattening to feed into DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "\n",
        "    #Hidden Layer with 512 neurons\n",
        "    tf.keras.layers.Dense(512, activation= 'relu'),\n",
        "\n",
        "    #Output layer\n",
        "    tf.keras.layers.Dense(6, activation= 'softmax')\n",
        "\n",
        "    ])\n",
        "\n",
        "model.compile(loss= \"mean_squared_error\", optimizer= RMSprop(lr=0.001), metrics = [\"acc\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CutP8yr48NoV",
        "outputId": "c87e7c69-7c98-44bd-8870-e1337d2870da"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fitting the model\n",
        "history = model.fit(train_data, \n",
        "                    epochs = 20, \n",
        "                    steps_per_epoch = 25, \n",
        "                    validation_data = valid_data, \n",
        "                    validation_steps = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOsoUEfT8N2o",
        "outputId": "39760b0d-b69b-4a9b-f8af-10103e65e3fd"
      },
      "execution_count": 51,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            " 5/25 [=====>........................] - ETA: 52s - loss: 0.1904 - acc: 0.1640"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - ETA: 0s - loss: 0.1534 - acc: 0.1946"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 5 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 70s 3s/step - loss: 0.1534 - acc: 0.1946 - val_loss: 0.1344 - val_acc: 0.2907\n",
            "Epoch 2/20\n",
            "25/25 [==============================] - 68s 3s/step - loss: 0.1297 - acc: 0.3016\n",
            "Epoch 3/20\n",
            "25/25 [==============================] - 64s 3s/step - loss: 0.1223 - acc: 0.3803\n",
            "Epoch 4/20\n",
            "25/25 [==============================] - 68s 3s/step - loss: 0.1216 - acc: 0.4218\n",
            "Epoch 5/20\n",
            "25/25 [==============================] - 63s 3s/step - loss: 0.1121 - acc: 0.4528\n",
            "Epoch 6/20\n",
            "25/25 [==============================] - 67s 3s/step - loss: 0.1030 - acc: 0.5171\n",
            "Epoch 7/20\n",
            "25/25 [==============================] - 63s 3s/step - loss: 0.0944 - acc: 0.5594\n",
            "Epoch 8/20\n",
            "25/25 [==============================] - 66s 3s/step - loss: 0.0855 - acc: 0.6221\n",
            "Epoch 9/20\n",
            "25/25 [==============================] - 64s 3s/step - loss: 0.0753 - acc: 0.6767\n",
            "Epoch 10/20\n",
            "25/25 [==============================] - 64s 3s/step - loss: 0.0598 - acc: 0.7419\n",
            "Epoch 11/20\n",
            "25/25 [==============================] - 66s 3s/step - loss: 0.0506 - acc: 0.7883\n",
            "Epoch 12/20\n",
            "25/25 [==============================] - 63s 3s/step - loss: 0.0377 - acc: 0.8420\n",
            "Epoch 13/20\n",
            "25/25 [==============================] - 66s 3s/step - loss: 0.0343 - acc: 0.8648\n",
            "Epoch 14/20\n",
            "25/25 [==============================] - 64s 3s/step - loss: 0.0213 - acc: 0.9235\n",
            "Epoch 15/20\n",
            "25/25 [==============================] - 67s 3s/step - loss: 0.0156 - acc: 0.9432\n",
            "Epoch 16/20\n",
            "25/25 [==============================] - 64s 3s/step - loss: 0.0205 - acc: 0.9243\n",
            "Epoch 17/20\n",
            "25/25 [==============================] - 67s 3s/step - loss: 0.0161 - acc: 0.9397\n",
            "Epoch 18/20\n",
            "25/25 [==============================] - 64s 3s/step - loss: 0.0111 - acc: 0.9601\n",
            "Epoch 19/20\n",
            "25/25 [==============================] - 67s 3s/step - loss: 0.0146 - acc: 0.9438\n",
            "Epoch 20/20\n",
            "25/25 [==============================] - 67s 3s/step - loss: 0.0067 - acc: 0.9780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The basics of the code for creating graphs of the training and validation accuracy per epoch came from the following tutorial provided by Google:\n",
        " https://colab.research.google.com/github/lmoroney/mlday-tokyo/blob/master/Lab6-Cats-v-Dogs.ipynb#scrollTo=dn-6c02VmqiN\n",
        " I added to the code to make the graphs more descriptive.\n",
        "'''\n",
        "\n",
        "# Retrieve a list of list results on training and test datasets for each training epoch\n",
        "acc=history.history['acc']\n",
        "val_acc=history.history['val_acc']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "# Get number of epochs\n",
        "epochs= range(len(acc)) \n",
        "\n",
        "# Plot training and validation accuracy per epoch\n",
        "plt.plot(epochs, acc, 'r', label = \"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, 'b', label = \"Validation Accuracy\")\n",
        "plt.title('Training and validation accuracy per epoch')\n",
        "plt.legend(loc= 2)\n",
        "plt.figure\n",
        "plt.show()\n",
        "\n",
        "print(\" \")   #Inserting a space between plots\n",
        "\n",
        "# Plot training and validation loss per epoch\n",
        "plt.plot(epochs, loss, 'r', label = \"Training Loss\")\n",
        "plt.plot(epochs, val_loss, 'b', label =  \"Validation Loss\")\n",
        "plt.title('Training and validation loss per epoch')\n",
        "plt.legend(loc= 0)\n",
        "plt.figure()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "JKPmJPdoqZXv",
        "outputId": "18d962f4-e7ac-4227-c9df-31e18033e2da"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-bd74ccc2658d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Plot training and validation accuracy per epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Training Accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Validation Accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training and validation accuracy per epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (20,) and (1,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deZhU5ZX/P4ddXEEJKIuAQNOoNDat0bjELYmaDMaICE7iNokbKjErSX5xMkyexGiixmUykIlGExXNRIVEjFtrMklQaVaFFmyRbhpQQUUUUWx4f3+cunZRVHffqrpb3T6f56mnqu56+va933rf8573HHHOYRiGYaSXLnEbYBiGYYSLCb1hGEbKMaE3DMNIOSb0hmEYKceE3jAMI+V0i9uAXA444AA3dOjQuM0wDMMoKxYuXLjJOdcv37rECf3QoUOpq6uL2wzDMIyyQkQa21pnrhvDMIyUY0JvGIaRckzoDcMwUo4JvWEYRsoxoTcMw0g5voReRE4TkZUi0iAi0/OsP0FEFolIi4hMzFl3vYgsF5F6EblFRCQo4w3DMIyO6VDoRaQrcDtwOjAGmCIiY3I2awIuBO7N2fdTwLHAWOAw4Ejg0yVbbRiGYfjGT4v+KKDBObfaObcdmA2cmb2Bc26Nc24ZsDNnXwf0AnoAPYHuwOslW20YaeTJJ6G+Pm4rjBTiR+gHAmuzvjdnlnWIc24+8DSwIfN6zDm3250sIpeISJ2I1G3cuNHPoQ0jfXzlKzB9N8+oYZRMqIOxIjICqAQGoT8OJ4vI8bnbOedmOedqnHM1/frlncFrGOlm2zZ47TVYtChuS4wU4kfo1wGDs74Pyizzw1nAs86595xz7wGPAscUZqJhdAKam1vf33gjXluM1OFH6BcAI0VkmIj0ACYDc30evwn4tIh0E5Hu6ECsOSENI5fGrDQl1qo3AqZDoXfOtQBXAo+hIv2Ac265iMwQkQkAInKkiDQD5wAzRWR5Zvf/BV4BXgCWAkudc38K4e8wjPKmqan1swm9ETC+slc65+YB83KWXZv1eQHq0sndbwdwaYk2Gkb6aWoCERg8GBYujNsaI2XYzFjDSAJNTXDggXD00daiNwLHhN4wkkBTExx8MFRXw5o18NZbcVtkpAgTesNIAo2NMGQIjB+v3xcvjtceI1WY0BtG3OzcCWvXqtAfcYQuMz+9ESAm9IYRNxs3wocfqtDvv7+6cMxPbwSICb1hxI0XWnnwwfo+frwJvREoJvSGETfeZKkhQ/S9uhpefhneeSc+m4xUYUJvGHHjteizhR5gyZJ47DFShwm9YcRNUxPstRfst59+94Te3DdGQJjQG0bcNDVpa94rvta/PwwcaJE3RmCY0BtG3HiTpbKprrYWvREYJvSGETfeZKlsxo+Hl16CrVvjsclIFSb0hhEn778PmzbtLvTV1eCcDcgagWBCbxhxsjZTpTOf0IO5b4xAMKE3jDjJDa30OOggHZQ1oTcCwITeMOIkd1ash4i26i3yxggAE3rDiJPGRujSRVvwuVRXw4oVWjjcMErAhN4w4qSpSUW+e/fd140fDzt2wAsvRG+XkSpM6A0jTrzJUvnwBmTNfWOUiAm9YcRJe0I/ZAj07WsDskbJmNAbRlx4BUdyB2I9RCxlsREIJvSGERevvw7bt7fdogd137zwghYmMYwi8SX0InKaiKwUkQYRmZ5n/QkiskhEWkRkYtbyk0RkSdbrAxH5YpB/gGGULW3F0GdTXQ0ffQTLl0djk5FKOhR6EekK3A6cDowBpojImJzNmoALgXuzFzrnnnbOjXPOjQNOBt4HHg/AbsMof/wIvVcs3Nw3Rgl087HNUUCDc241gIjMBs4EVngbOOfWZNbtbOc4E4FHnXPvF22tYaQJP0I/fDjsu68JvVESflw3A4G1Wd+bM8sKZTJwX74VInKJiNSJSN3GjRuLOLRhlCGNjbDPPq0FR/IhAkccYSGWRklEMhgrIgcChwOP5VvvnJvlnKtxztX069cvCpMMI37aC63MZvx4WLpUffWGUQR+hH4dMDjr+6DMskKYBDzknLM71TA8/Ap9dbVG3bz0Uvg2GanEj9AvAEaKyDAR6YG6YOYWeJ4ptOG2MYxOSyFCD+a+MYqmQ6F3zrUAV6Jul3rgAefcchGZISITAETkSBFpBs4BZorIx7FgIjIU7RH8NXjzDaNM2boV3nyz7clS2YwapcXDbUDWKBI/UTc45+YB83KWXZv1eQHq0sm37xqKG7w1jPTiJ+LGo0sXGDfOhN4oGpsZaxhxUIjQg7pvFi/WbJaGUSAm9IYRB8UI/fvvw6pV4dlkpBYTesOIg6Ym6No1f8GRfNgMWaMETOgNIw6ammDgQOjma5gMRo+GXr0s8sYoChN6w4iDxkb/bhvQH4SqKmvRG0VhQm8YceA3hj6b8eN1QHZneymlDGN3TOgNI2p27IDm5sKFvroatmyBV14Jxy4jtZjQG0bUvP665q0pRujB3DdGwZjQG0bUNDbqu59Zsdkceij06GFCbxSMCb1hRE2hMfQePXrA4Ydb5I1RMCb0hhE1xQo9qPtm0SJwLlibjFRjQp8kHnxQ/bdGumlq0qpR++xT+L7jx8Pbb7e6f4xWamthyZK4rUgkJvRJYdMmOPtsuOWWuC0xwqaY0EoPS1m8Ox99BNdcA6ecApdfHrc1icSEPinU1+v7ihXtb2eUP42NhQ/Eehx+uE6esgFZZd06OOkkuPlmGDQIli2zxG95MKFPCp7AWxWh9FNKi75XL42+MaGHp59uzep5773wn/+pid8aGuK2LHGY0CcFr0Xf0GC1QdPMu++qj71YoQcVt4ULO++ArHPws5/BqadC376wYAFMmaIpIkDr6xq7YEKfFLwWfUuLtUjSzNq1+l6q0G/cqG6LzsbmzXDWWTB9OkycCM8/D2PG6LoxY9StZUK/Gyb0SaG+HiorWz8b6aSU0EqPzpqyeOlSqKmBRx5Rn/zs2bD33q3re/bUZ8iEfjdM6JPAli2a++Sss/S7CX16KXZWbDZjx2p5wc4k9HfdBUcfDdu2wTPPwLRpILL7dlVVFmKZBxP6JOANwNbUaEvPhD69eAVHDjyw+GPsuafmp+8MIZYffACXXgoXXgjHHKM/bsce2/b2VVXq0nrzzchMLAdM6JOAJ+xjxmjX0yJv0ktTk4YBdu1a2nHGj09/i37NGjjuOJg1S33yjz8O/fu3v48NyObFl9CLyGkislJEGkRkep71J4jIIhFpEZGJOeuGiMjjIlIvIitEZGgwpqeI+nro3h0OOURbai+9ZDnH00opoZXZVFfD+vXw2mulHyuJPPqo/o0NDfDww/DTn/qrxuUJvblvdqFDoReRrsDtwOnAGGCKiIzJ2awJuBC4N88h7gZucM5VAkcBb5RicCpZsQJGjtQbubIStm5Vn72RPkqZLJVNWlMW79gB//7v8PnPw+DBUFcHZ57pf/9PfELdYtai3wU/LfqjgAbn3Grn3HZgNrDLlXfOrXHOLQN2aYZmfhC6OeeeyGz3nnPu/WBMTxH19a0hYhZ5k16KLTiSjyOO0Pc0Cf2mTXDGGTBjBpx/PsyfDyNGFH6cqioT+hz8CP1AYG3W9+bMMj+MAjaLyIMislhEbsj0EAyPDz6A1atbBd6EPr1s2KBiH4TQ7703jBqVLqH/ylc0ombWLLjzTujdu7jjjBunveTt2wM1r5wJezC2G3A88C3gSGA46uLZBRG5RETqRKRu48aNIZuUMFatUn+816Lv1w/239+EPo0EEUOfjTdDNi0sWQJf/jJ87Wv5Qyf9UlWls8vtGfoYP0K/Dhic9X1QZpkfmoElGbdPC/AwUJ27kXNulnOuxjlX069fP5+HTgnezei15L3PdpOmj6CFfvx4PeamTcEcL04+/FAHloMYv7DIm93wI/QLgJEiMkxEegCTgbk+j78A2E9EPPU+GbD0jNmsWKGtl1GjWpd5kTdGuvAmSwXZogdN6lXueMEHQVybkSM1+ZsJ/cd0KPSZlviVwGNAPfCAc265iMwQkQkAInKkiDQD5wAzRWR5Zt8dqNvmKRF5ARDg1+H8KWVKfT0MHw577NG6rLJSc5nYpI900dQEffrsOm2/FLwB2TS4b4Ls7XTrpumcLcTyY3wEpoJzbh4wL2fZtVmfF6AunXz7PgGMLcHGdJOd48Yje0D2uOOit8kIh6Bi6D369NFGQhoGZD2hD8J1A+q+eeghzXRZir8/JdjM2DhpaYGVK1sHYj0s8iadBC300FpDttzxhH5Q3vZi4VRVaY94/fpgjlfmmNDHyerVGh2Q26IfMkRDy0zo00VYQv/KK5q+t5xpbIQBAzQDZRCMG6fv5qcHTOjjxctBnyv0XbpARYUJfZrYskXFOCjXhIeXsrjcB2SD/hEcm/EWm58eMKGPl3yhlR4WeZMugg6t9EjLDNmghX6ffWDYMGvRZzChj5P6ehg4UG/KXCortTv7vmWMSAVhCX2/fpoTppwjb5wLx601bpwJfQYT+jhZsWL3gViPykp9AFaujNYmIxzCEnoo/5TFb76pBUWCvjZVVTrzfOvWYI9bhpjQx8XOneqayee2AYu8SRuNjRrfPWBA8MeurlZBe/fd4I8dBWH9CFZVaWPpxReDPW4ZYkIfF83N2tJoS+hHjtTiFCb06aCpSV0spRYcyUd1tQpauQ48hin0YO4bTOjjw4u4act106OHFiIxoU8HYfigPcq9WHhYQj90qI5/mdCb0MdGexE3HlZWMD2EKfQDBmixjXIW+j32gAMOCPa4IlYsPIMJfVysWKHpiNvL1jl6tPpeW1qis8sInpYWLVgdltBDec+Q9X4Ew0hVUFUFy5Z1+tKcJvRxkV1Vqi0qK3Xm7OrV0dhkhMP69VpwJOjJUtlUV2vjoRzDccPs7YwbB++9B6++Gs7xywQT+jhwTh/K9tw2YJE3aSHM0EqP8eO11bpsWXjnCIvGxvCujRULB0zo4+GNN+Dttztu0Y8ere8m9OVNFEJfrsXCvYIjYV2bQw/VlCKdfEDWhD4O/AzEgkYMDBxoQl/ueEI/eHD725XCoEE6mFluQh9kwZF87LGH5o0yoTcip61kZvmwsoLlT1MT9O0Le+0V3jlE4LDDWu+tciGK3s64cea6iduATkl9vT70fnJve8nNnAvfLiMcGhvDHYj1qKgov3slCqGvqtLzvP12eOdIOCb0ceBVlfITTlZZqVPbrYBC+RJmVEk2o0ermJVTsfCgC47kwxuQLceB6oAwoY+D9pKZ5WKRN+VPVEJfUaHv5ZQIr6kJ+vfXYt5hYUVITOgj5513YMMGf/55MKEvd955R4uORNWih/KaTd3UFL5ba8AA+MQnOrWf3oQ+avxG3Hj07w/77WdCX65E4YP2GDJEW8bl1qKP4tpUVVmL3oiQjpKZ5SJikTflTGOjvkcxGNu1q2Y9LZcWfVgFR/IxbhwsX64zzTshvoReRE4TkZUi0iAi0/OsP0FEFolIi4hMzFm3Q0SWZF5zgzK8bKmv1wLIw4b538fKCpYvUbboQe+VcmnRv/WWpmyIqkX/4Yflc20CpkOhF5GuwO3A6cAYYIqI5DZHm4ALgXvzHGKbc25c5jWhRHvLnxUrdNCskLzklZU6e3Dz5vDsMsKhqQm6d1cXXBRUVGhupO3bozlfKXi9naiEHjqt+8ZPi/4ooME5t9o5tx2YDZyZvYFzbo1zbhnQuVPE+cELrSwEG5AtX7yCI10i8pKOHq0J1F55JZrzlUKUvZ2KCq3xYELfJgOBtVnfmzPL/NJLROpE5FkR+WJB1qWN99+HNWtM6DsTYSbsyocXYlkOrr4ohb57d505bEIfGgc752qA84CbReSQ3A1E5JLMj0Hdxo0bIzApJlau1AEovwOxHkOHql/fhL78iCJ8MJtyiqVvatIooaALjrRFJy5C4kfo1wHZ2ZgGZZb5wjm3LvO+GngGOCLPNrOcczXOuZp+7RXiKHcKDa306NpVH2AT+vLio490RnOULfq994aDDiqfFn1YBUfyUVWlmWNfey2a8yUIP0K/ABgpIsNEpAcwGfAVPSMifUSkZ+bzAcCxQJllXQqQ+nr11Y4cWfi+FnlTfqxfrznioxR6KJ/Im6hCKz068QzZDoXeOdcCXAk8BtQDDzjnlovIDBGZACAiR4pIM3AOMFNElmd2rwTqRGQp8DRwnXOu8wr9ihUwYoS6YQqlslKr5HzwQfB2GeEQdWilR7kkN4varTV2rL53QvdNNz8bOefmAfNyll2b9XkB6tLJ3e+fwOEl2pgeiom48ais1NbhqlWtN6yRbKIMH8xm9GgNxd24Uaf+J5EPP9RUIFFemz599HxJbdE7pwkM99kn8EPbzNio+OgjePnlwgdiPdIeeeMcfOYzMHNm3JYER5wteki2q29dZpgv6muT1FQIq1bBKafAOeeE0hMzoY+KhgZoaSm+RT9qlPr30yr0K1fCk0/Cf/xHeUz28UNTk0aU9O4d7Xm95GZJ9tPH9SM4bpxel23boj1vW2zfDj/+sfbSFy2Cs88O5TQm9FFRbMSNR69emjYhrUJfW6vvGzbAH/8Yry1BEfVgo8fgwVpCL8kt+riEvqpKJ5QtX97xtmHzj3/AEUfAD38IZ56pz/Yll4QShWRCHxVeMjOvtVUMo0enW+gHD9aIpFtuiduaYIhL6Lt00R5gklv03vhFmAVH8pGEVAibN8Nll8Fxx8F778Gf/wz33w8HHhjaKU3oo6K+Xh/6UuqGVlaqL2/HjuDsSgI7d8LTT6uP8qqr4Nln4fnn47aqNJyLroRgPrzIm6QSRcGRfAwfrs9gHELvHPzhD/oc//rXcM012rP4/OdDP7UJfVTU1xc/EOtRWanRCmvWBGJSYli6VDMZnnwyXHCBTvop91b95s3aWoujRQ/a+3v1Vb1fkkicvZ2xY6MPsWxqggkTYNIkbbk//zzceGO4BeOzMKGPgp07tXVVrH/eI62RN55//uSTNbTs4ovhgQfUX1+uxOWD9qio0PuuoSGe83dEXEIP6r5ZtiyaeQY7dsDNN2sjr7YWfvELFfnx48M/dxYm9FHQ2Kij/Cb0+amtVWEamMmVd+WVGqH03/8dr12lELfQJ7msYJQFR/IxbpyWePTGCcJi8WI4+mh10ZxwgrppvvEN6OZr+lKgmNBHQaFVpdpiv/20/mWahP6jj+Bvf9PWvMeIEXDGGSr0SXU9dERck6U8Ro3S9yQOyEZZcCQfYQ/Ibt0K3/oWHHkkrF0Ls2fDI49ocsKYMKGPglJDK7NJW+RNXZ36srOFHmDaNE1A9cAD8dhVKk1Nmuoirpmpe+2lES1JbNF7vZ24BqoPO0xDGMPw0z/6KBx6qLpoLr5Yn9Vzz40ucVsbmNBHQX29PvB9+5Z+rMrK8shj4penntL3E0/cdfmpp+rf+stfluffGnXBkXxUVCSzRR+3W2vPPTWMN+gW/Xe+oz3R3r21lzprlqZdSAAm9FGwYkXpbhuPykqN6Hj99WCOFze1tdqVzs1JLqKhlgsXwvz58dhWCnH6oD28jKdJ+6GMW+hB/fRBCv2zz8INN8BFF6lv/vjjgzt2AJjQezgH//Vf0Nwc/HFLSWaWS5oGZLdtg3/+U+Pn83H++bDvvtqqLzeSIPQVFbBlS/IaBVEXHMlHVZXW1t2ypfRj7dihAQQHHqj3ajHZaUPGhN5j8WKYOhW+//1gj7thg47wB9mih3QI/fz5Otia65/32HNP+OpXNSVC0D/AYbJ9e/QFR/KR1MibqAuO5MMbkF22rPRj3XGH9jxvuEHngCQQE3qPOXP0ffbsYCvQBDkQC1o9aO+90yH0tbVaPau9bu7Uqdor+tWvorOrVNatU5vjGmz0SGpZwajr6OYjqCIkb70F3/uepjM477zS7QoJE3qPOXPgkEM03C/IVLlBC71IeiJvnnpKQ9Day789bJjOKJw5MzkZBzsiCT5o0Kib3r2T26KPk4MOgv33Lz3y5tpr4e234bbbYo+saQ8TetAWxtKlmmjojDO09RhUqtwVK9TPHGTCIi/yppzZsgUWLGjbP5/N1VfDm2/CffeFb1cQJEXok5jcLI6CI/kQKT03/dKlqhWXX97qCkooJvQAczMlcCdMUFF5/XVNPhQE3kBskL/2lZXqHghiICku/u//dBCrLf98NieeqLHPt9ySvAiSfHhCP3hwvHZA8moNx1VwJB9VVfDii8UlCXROo8L69oUZM4K3LWBM6EHdNqNHa+vnM59R32ZQ8dtBJDPLxXMDJekBLpTaWo1OOOaYjrcV0QlUS5dqfHLSaWyEfv00J3zcVFRoEryk1BpOSm8H1E+/bZtWfiuU++7TxspPfhLM/JiQMaHfvBn++ldN/A/a3b36anUrPPdcacd+6y3tHQTln/dIQ+RNbS186lP+xfC88/SBKoesllEXvW6P0aO1wVKMmIVB3LNis/HcLYX66d99V1Mc1NTo7NcywIR+3jxNoOUJPbTGb5cqKkEPxHoMHw7du5ev0G/apA+XH7eNR+/e8LWvwcMPh5+MqlSSMNjokbTIG0/ooy44ko/KSn2OCvXT//jHOs5w220aNVYGmNDPnavpCT75ydZle+0F//Zv6qf3fIrFEFQys1y6ddMp3OUq9M88o+9+BmKzueIKdePcfnvgJgVG3JkZc/GSmyXFzRdXwZF89Oihz2YhQr9yJdx0k86AzdaMhONL6EXkNBFZKSINIjI9z/oTRGSRiLSIyMQ86/cRkWYRuS0IowNj+3ZNQvQv/7J7TpKpU3WQppRUufX16poIo5taWVm+Ql9bqz+mNTWF7TdkCJx1llbn2bo1HNtK5a231LakCP2ee+qgcJJa9Em5NqDuG7+uG+fUrbvHHvDTn4ZrV8B0KPQi0hW4HTgdGANMEZHcJmoTcCFwbxuH+U8geaNozzyjkSvZbhuP4cP1B2DmzOIHsurr1UcaRmKrykqdwh1kGt+oBu1qazU/d/fuhe87bZqOq9xzT/B2BUGSBhs9khR5k0Sh37ABNm7seNs5c+DxxzXKpn//8G0LED8KdBTQ4Jxb7ZzbDswGdlFG59wa59wyYGfuziIyHugPPB6AvcEyZ476fk89Nf/6adP0Bpg9u7jjr1gRvH/eo7JSexxBVRBqaFBB+O53gzleW6xbp63LQvzz2Rx7LBxxRDihlps364S5UkjSYKOHl8Uy7tDUpLm1wP8M2W3btIDIYYdpb7/M8CP0A4G1Wd+bM8s6RES6AL8AvlW4aSHjnPrnP/vZtiM/TjpJc0sXIyrvvac3dZhCD8G5b77+de0d3HFHuPH52WUDi0FEu8/Ll7ceKwj+8Ad1cdTUlBahktQW/bvvxl+aMWluLfBfhOT667XHe+utsVSIKpWwB2OvAOY559rNSCUil4hInYjUbfTThQqCxYs1UdaECe0ZpqKyeDH84x+FHd/ziQY9EOvhRVMEIfSPPKKvyZP1B+p3vyv9mG1RW6thkqXMJJw8WePUgwi1/OgjLe82aZJe0+ZmFfuHHy7ueF7BkX79SrctKJISeZPEH8H999cSlu356desgeuu0wIiuXUTygQ/Qr8OyJ7iNyizzA/HAFeKyBrg58D5InJd7kbOuVnOuRrnXE2/qB6QOXPUd/6FL7S/3Ze/rMUDCk2V60XchNWi791b3QOlCv2HH2prvqIC7rpLixbffns43XznVOhPOqm0cYteveDSS+FPf4JXXin+OOvXqy033aSzHP/5T81COGqUDvp+97saelsIXsKuJOU9SUoWyyQKPXScCuEb39D79ec/j86mgPHztC0ARorIMBHpAUwG5vo5uHPuX51zQ5xzQ1H3zd3Oud2idmJhzhydsNPRD4sXv/3QQ603qh/q67WLN2JEaXa2RxCRNzfdpP75W27RcLOpU/WYXghkkKxerdewWLdNNpdfrjHMxYZaPvOM+voXL4Z77239+4cOhb//XfMeXX+9zpQuJJtpkiZLeQwcqNE31qLPz7hxes/nC2x4/HF99v/f/0tG7H+RdCj0zrkW4ErgMaAeeMA5t1xEZojIBAAROVJEmoFzgJkisjxMo0tmzRr9Bc8XbZOPK64oPFXuihUa615MZIlfKiv14d252xi4P9at08kfX/yijlWAukX69g0nVr1U/3w2Bx0E55wDv/mNupv84pwK+Cmn6N+5YAFMmbLrNj176v/6rrt0dnR1tYq/H5I22Ajau6ioSEaLvlevZLm1QFv0LS2tvXCP7dvVdTtihLbqyxnnXKJe48ePd6Fzyy3OgXOrVvnf5+yznevb17mtW/1tP2qUc1/6UnH2+WXWLP07Xn21uP2nTHGuVy/nVq/edfm3v+1c167OrV1bsom7cO65zh10kHM7dwZzvPnz9e+/7TZ/22/e7NwXv6j7TJrk3JYtHe+zdKlzI0bo9bjxxvZt/+ADPfaPfuTPnig57zznhg6N14ZJk/S5SBovvaT/tzvv3HX59dfr8kceicWsQgHqXBu62jlnxnpJzEaO9L/P1Vdr1MC9bU0VyOLDD9V3HNZArEcpkTd/+5smZvrOdzTnezaXX669hCDz8nv++ZNPDs5//clPaj77W2/tuFezbJkOsv75z3DzzRoy66ca0NixUFencyq8Qdt3382/bZIyM+ZSUaHjB3Hm9E9ibwe0xd67965++vXrNV7+C1/Q1OVlTucT+twkZn45/njt4vkJtXz5ZY1xD2sg1qNYoW9p0cHHIUPyx80PG6Y3969/HVxe/uXLdU5CEG4bDy+r5cqV8MQTbW93991w9NHw/vvqm582rbAfm333hQcfVJfPgw/qj8vyPN5JLwdPEsUsCcnNkir0XbvC4YfvKvTf+Y7e+zffHJ9dAdL5hD5fEjM/eKLywgsdD1SGlcwsl/331wLLhQr9zJnawr3xRm3J5GPqVM28+cc/lm4nBOufz+acc2DAgPxRUR98oIOqF1ygQr9okU64KgYR+Pa3tSrW5s1w1FG7F0JJ4mQpDy/EMi4//fbtySg40hZeKgTnNP3wPffo//uQQ+K2LBA6n9DPnavTl4tJSDRligprR/Hb9fWtA2BhU2jkzaZN8MMf6mDkl77U9naf+5ze5EENyj71lKaVCFoEe/RQMX/0UVi1qhg+zbkAABJZSURBVHX5mjXaC5s5E6ZP1+iJIKatn3ii/mBUV2vq5Kuuau31JCkzYy4jR+o9GVfkjVdHN8lC//bb2iu76iqdPPe978VtVWB0LqFvL4mZH3r1gksu0R+LV19te7sVKzRMr63WcpAUWlbwBz/Qma+33NK++6JLF402+sc/Sq+r2dKivaBCs1X65bLLNLrptkzOvL/8RecDvPyyTnz66U+Dnc140EHaQ/nmN/Wcn/40rF2brMyMufTurSIbV4s+qaGVHl4qhKlT1YVz440akpoSOpfQe0nM2psN2xF+UuWGUVWqLSortZ6qnxnFCxeq3/2qq/zZd9FFmh6i1Fb94sV63YN223j0769hoXfeqa2wM87QVnVdXeEuOr90764TaP7wB/XXV1fD008nV8gg3uRmSR6/APXRg7p2Tz4Zzj47XnsCpnMJfUdJzPwwcCBMnAj/8z/547d37NDucdj+eQ+/A7I7d6rA9+sHP/qRv2P36aPuiXvu0W5tsXj++ZNOKv4YHXH11fr/uO46LRwzf364k9U8Jk7UWPz+/TXSKqlCBvEmN0uyWws0AuuQQ7Tnd+utyZrZHACdR+j9JDHzy9VXwzvvwO9/v/u6V1/V8MqkCf3vf6/id911GkXil6lTNSTvt78t2kRqazU5XJipXWtq4NprtVV/553RuM08Kip0YtX06epGSiqjR2tSsVKK6RRLU5MW+ElCHd22+P731RUXVW88StoKsI/rFdqEqYUL80+KKIadO52rqXGusnL3CTRz5+p55s8v/Tx+2LHDud69nZs2re1t3nnHuf79nfvkJ3X7QjnmGJ00VMy+H3zg3B57OHfVVYXvawTLU0/pvfnkk9Gf+3Of02fGCA1swhStScw+//nSj+Vltayvhyef3HVd2MnMcunSRVtq7bXoZ8yAN97QLmkxg9BXXqn5cNqLVW+L557THkFYA7GGf+JMbpbEHECdiM4l9H6SmPll0iR1ReSGWtbXa1RGIe6RUmkvxLK+XmPML75YJ/oUw9lna7e7mEHZ2lr9cfn0p4s7txEcBx6oJRyjDrFMYsGRTkbnEPpCk5j5oWdP9cc+8siuVZ7CrCrVFpWVGt6XOzjsnE7y2nNP+MlPij9+z56awfPPf24/rDQftbUakbLffsWf3wgGkXgib95+O3kFRzoZnUPo//QnfQ861O6yy3SU3ovfdk4fojiEHnZvqT38sLpbZszQFnkpXHqpCkUhxdK3boVnnw0vrNIoHC/yJkqSHkPfCegcQj9njophIUnM/DBggFadueMOTXS1bp2+Rz1qny/yZts2TcJ12GEa+18qgwdrOuPf/MZ/AfG//10rOJl/PjmMHq3Cu3VrdOc0oY+d9Au9l8SslElS7XH11Sruv/1t9AOxHoccoomZsoX+hhuCr3E5dapOzrr/fn/b19bqxKJi88sYweOl5YgyuZkJfeykX+iLTWLmlyOP1IRZt94an9D36KGTgzyhb2zUaf+TJgVb4/Kkk/Rv8zsoW1ur1yZFU8nLnjgib5JYR7eTkX6hnzOn+CRmfpk2TVtIt9+uVYtK9YcXQ3bkzTe/GU6NSxF1Ay1YAM8/3/62b7+tyb/MP58sRoyIPrlZEuvodjLSLfSlJjHzy9lna0hlQ4MKbhw3dGWlnv8vf9HUwt//vvrVg+b88zVEr6NW/d/+pmkXTOiTxR57aMK9qFv05raJlXQL/TPPqP88LP+8R/fuWpUJ4ps+XVmpLqoLLtB0wN/8Zjjn2WcfFfv779eUx23x1FMqKkcfHY4dRvFEHXljQh876Rb6IJKY+eXSSzVW/FOfCv9c+fDGBd54Q6vihJkq94orNJ/Pb37T9ja1tZoPvkeP8OwwimP06NKKyheCV3DEZsXGSnqFPsgkZn7o1w9ee01b1HFQUaHuqdNO0zqXYXLooTrI+6tfabbOXF5/XVP3mtsmmVRUaFnFKJKbJb3gSCchvUK/eDE0N4cXbZOPnj3jG3Dae2+tovS730Vjw9SpOsg2b97u655+Wt9N6JNJlJE3FlqZCHwJvYicJiIrRaRBRKbnWX+CiCwSkRYRmZi1/ODM8iUislxEosvhGmQSs3LhlFO01GEUnHmmDkB7s4Kzqa3VXD/V1dHYYhSGF0sfhZ/ehD4RdCj0ItIVuB04HRgDTBGR3BHHJuBC4N6c5RuAY5xz44BPAtNF5KBSjfZF0EnMjF3p3l1TQDz++K61WkEHYk88USdxGcljwAAdVI+yRZ/UgiOdBD8t+qOABufcaufcdmA2sIs/xDm3xjm3DNiZs3y7c+7DzNeePs9XOmEkMTN252tfU8H/1a9al61ZA6tXm9smyXiF66Nq0Se94EgnwI/wDgTWZn1vzizzhYgMFpFlmWP8zDm3vjATi2DuXH03oQ+XAQN0DsGdd7bmTjH/fHkQVRZLC61MBKG3sJ1za51zY4ERwAUisls9ORG5RETqRKRuo58i1x0xd244ScyM3Zk6Vcsq3pvx2tXWagvu0EPjtcton4oKDVbIV/c4SEzoE4EfoV8HZE+xHJRZVhCZlvyLwPF51s1yztU452r6lepTDzuJmbErxx4LY8fqoKxzKvQnn2zT3ZOOF3mTO74SJM61pj8wYsWP0C8ARorIMBHpAUwG5vo5uIgMEpE9Mp/7AMcB4ToGw05iZuyKiJYaXLZM0zWvX29um3IgisgbKziSGDoUeudcC3Al8BhQDzzgnFsuIjNEZAKAiBwpIs3AOcBMEVme2b0SeE5ElgJ/BX7unHshjD/kY6JIYmbsynnnaTjlNdfodxP65DNihIYfh+mn9yJubFZs7PhKVO6cmwfMy1l2bdbnBahLJ3e/J4CxJdroHy+J2bnnhpvEzNiVPfeEiy7S1AtDhmiuHSPZ9Oqlyc3CbNFbDH1iSJcaeknMzG0TPV4VK/PPlw9hR96Y0CeGgEoPJQQviZmVrouekSPhwQfhiCPitsTwS0WFhsPu3BlOD9gKjiSG9Ah91EnMjN0566y4LTAKYfRorS28dm04fnQvtNJ6eLGTHtfNmjU6ym9uG8PwR9iRNxZDnxjSI/TDhmkhjHPPjdsSwygPws5iaUKfGNLjuoFwi20YRtr4xCc0LDYMof/oI51TYUKfCNLTojcMozBEWqtNBY0VHEkUJvSG0ZmpqAinRd/YqO8m9InAhN4wOjOjR6uL5d13gz2uxdAnChN6w+jMeAOyQbtvPKEfPLj97YxIMKE3jM5MWCGWVnAkUZjQG0Zn5pBDtORj0H56C61MFCb0htGZ6dlT56CE0aI3oU8MJvSG0dkJOrmZcyb0CcOE3jA6OxUV8PLLsGNHMMfbvFlLFJrQJwYTesPo7IweDR980BopUyoWWpk4TOgNo7MTdOSNCX3iMKE3jM5O0MnNTOgThwm9YXR2DjgA+vQJrkXf2GgFRxKGCb1hdHa85GZBtuiHDLG6zQnC/hOGYaifPkgfvbltEoUJvWEY2qLfsAG2bCn9WCb0icOX0IvIaSKyUkQaRGR6nvUniMgiEWkRkYlZy8eJyHwRWS4iy0TEyj8ZRhIJKvLGCo4kkg6FXkS6ArcDpwNjgCkiMiZnsybgQuDenOXvA+c75w4FTgNuFpH9SjXaMIyACSryxgqOJBI/pQSPAhqcc6sBRGQ2cCawwtvAObcms25n9o7OuVVZn9eLyBtAP2BzyZYbhhEcw4drcrNSW/QWWplI/LhuBgJrs743Z5YVhIgcBfQAXil0X8MwQqZHDxgzBu64A+bPL/44JvSJJJLBWBE5EPgdcJFzbmee9ZeISJ2I1G3cuDEKkwzDyOXuu6FXLzjhBLj1VnXBFIoVHEkkfoR+HZD9XxuUWeYLEdkHeAT4gXPu2XzbOOdmOedqnHM1/WyShWHEw7hxsHAhnH46XH01nHeeJicrhKYmnShlBUcShR+hXwCMFJFhItIDmAzM9XPwzPYPAXc75/63eDMNw4iEPn3g4YfhJz+BBx6Ao44qbIDWQisTSYdC75xrAa4EHgPqgQecc8tFZIaITAAQkSNFpBk4B5gpIsszu08CTgAuFJElmde4UP4SwzCCoUsX+N734PHHYdMmOPJIFX0/NDbCwQeHa59RMOKK8cOFSE1Njaurq4vbDMMwAJqbYdIkHaD9+tfh+uuhe/f82zoH++wDX/0q3HRTtHYaiMhC51xNvnU2M9YwjLYZNAieeUZ99jffDCedpLHy+bCCI4nFhN4wjPbp0QN++Uu47z5YsgSqq+Hpp3ffzkIrE4sJvWEY/pg8GZ5/Hvr2hVNPhZ/9bNcQTBP6xGJCbxiGf8aMUbGfOBGmT4ezzlKXDZjQJxgTesMwCmPvvWH2bPXZP/II1NTA0qUq9FZwJJGY0BuGUTgiMG2aDtRu2wZHHw0PPqgzYq3gSOKw/4hhGMVz7LGwaBEccww0NJjbJqH4yV5pGIbRNv376+SqW29VH76ROEzoDcMonW7d4Jpr4rbCaANz3RiGYaQcE3rDMIyUY0JvGIaRckzoDcMwUo4JvWEYRsoxoTcMw0g5JvSGYRgpx4TeMAwj5SSuwpSIbAQaSzjEAcCmgMwJA7OvNMy+0jD7SiPJ9h3snMubUS5xQl8qIlLXVjmtJGD2lYbZVxpmX2kk3b62MNeNYRhGyjGhNwzDSDlpFPpZcRvQAWZfaZh9pWH2lUbS7ctL6nz0hmEYxq6ksUVvGIZhZGFCbxiGkXLKUuhF5DQRWSkiDSIyPc/6niJyf2b9cyIyNELbBovI0yKyQkSWi8i0PNucKCLviMiSzOvaqOzLsmGNiLyQOX9dnvUiIrdkruEyEamO0LaKrGuzRES2iMjXc7aJ9BqKyB0i8oaIvJi1rK+IPCEiL2fe+7Sx7wWZbV4WkQsitO8GEXkp8/97SET2a2Pfdu+FEO37kYisy/ofntHGvu0+7yHad3+WbWtEZEkb+4Z+/UrGOVdWL6Ar8AowHOgBLAXG5GxzBfDfmc+TgfsjtO9AoDrzeW9gVR77TgT+HPN1XAMc0M76M4BHAQGOBp6L8f/9GjoZJLZrCJwAVAMvZi27Hpie+Twd+Fme/foCqzPvfTKf+0Rk32eBbpnPP8tnn597IUT7fgR8y8f/v93nPSz7ctb/Arg2rutX6qscW/RHAQ3OudXOue3AbODMnG3OBO7KfP5f4BQRkSiMc85tcM4tynx+F6gHBkZx7oA5E7jbKc8C+4nIgTHYcQrwinOulNnSJeOc+xvwVs7i7PvsLuCLeXb9HPCEc+4t59zbwBPAaVHY55x73DnXkvn6LDAo6PP6pY3r5wc/z3vJtGdfRjsmAfcFfd6oKEehHwiszfrezO5C+vE2mRv9HWD/SKzLIuMyOgJ4Ls/qY0RkqYg8KiKHRmqY4oDHRWShiFySZ72f6xwFk2n7AYv7GvZ3zm3IfH4N6J9nm6Rcx4vRHlo+OroXwuTKjGvpjjZcX0m4fscDrzvnXm5jfZzXzxflKPRlgYjsBfwR+LpzbkvO6kWoK6IKuBV4OGr7gOOcc9XA6cBUETkhBhvaRUR6ABOAP+RZnYRr+DFO+/CJjFUWkR8ALcA9bWwS173wK+AQYBywAXWPJJEptN+aT/yzVI5Cvw4YnPV9UGZZ3m1EpBuwL/BmJNbpObujIn+Pc+7B3PXOuS3Oufcyn+cB3UXkgKjsy5x3Xeb9DeAhtIucjZ/rHDanA4ucc6/nrkjCNQRe99xZmfc38mwT63UUkQuBLwD/mvkx2g0f90IoOOded87tcM7tBH7dxnnjvn7dgC8B97e1TVzXrxDKUegXACNFZFimxTcZmJuzzVzAi26YCNS2dZMHTcaf9xug3jl3YxvbDPDGDETkKPT/EOUP0Z4isrf3GR20ezFns7nA+Znom6OBd7LcFFHRZksq7muYIfs+uwCYk2ebx4DPikifjGvis5lloSMipwHfASY4595vYxs/90JY9mWP+ZzVxnn9PO9hcirwknOuOd/KOK9fQcQ9GlzMC40IWYWOxv8gs2wGekMD9EK7+w3A88DwCG07Du3CLwOWZF5nAJcBl2W2uRJYjkYQPAt8KuLrNzxz7qUZO7xrmG2jALdnrvELQE3ENu6JCve+Wctiu4boD84G4CPUT/xv6LjPU8DLwJNA38y2NcD/ZO17ceZebAAuitC+BtS/7d2HXiTaQcC89u6FiOz7XebeWoaK94G59mW+7/a8R2FfZvlvvXsua9vIr1+pL0uBYBiGkXLK0XVjGIZhFIAJvWEYRsoxoTcMw0g5JvSGYRgpx4TeMAwj5ZjQG4ZhpBwTesMwjJTz/wE/0OUgQmCZoAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Training and Testing Data for other Classifierss\n"
      ],
      "metadata": {
        "id": "6QO8cintSyT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#reindexing the dataframe containing the training data\n",
        "train_df.reset_index(drop = True, inplace = True)\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "#reindexing the dataframe containing the testing data\n",
        "test_df.reset_index(drop = True, inplace = True)\n",
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFYFA2I9SynE",
        "outputId": "0aa888c2-5ce7-404f-8108-930cc4defa5f"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  import sys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleaning out all pictures in training and testing dataframes that are not RGB (may be greyscale or have 4 dimensions)\n",
        "\n",
        "print(len(train_df))\n",
        "print(len(test_df))\n",
        "\n",
        "counter = 0\n",
        "while counter < len(train_df):\n",
        "  img = imread(train_df.filepaths[counter])\n",
        "  if len(img.shape) != 3:\n",
        "    train_df = train_df.drop([counter]) \n",
        "    train_df.reset_index(drop = True, inplace = True)\n",
        "  counter += 1\n",
        "print(len(train_df))\n",
        "\n",
        "counter = 0\n",
        "while counter < len(train_df):\n",
        "  img = imread(train_df.filepaths[counter])\n",
        "  if img.shape[2] != 3:\n",
        "    train_df = train_df.drop([counter])\n",
        "    train_df.reset_index(drop = True, inplace = True)\n",
        "  counter += 1\n",
        "print(len(train_df))\n",
        "\n",
        "counter = 0\n",
        "while counter < len(test_df):\n",
        "  img = imread(test_df.filepaths[counter])\n",
        "  if len(img.shape) != 3:\n",
        "    test_df = test_df.drop([counter]) \n",
        "    test_df.reset_index(drop = True, inplace = True)\n",
        "  counter += 1\n",
        "print(len(test_df))\n",
        "\n",
        "counter = 0\n",
        "while counter < len(test_df):\n",
        "  img = imread(test_df.filepaths[counter])\n",
        "  if img.shape[2] != 3:\n",
        "    test_df = test_df.drop([counter])\n",
        "    test_df.reset_index(drop = True, inplace = True)\n",
        "  counter += 1\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxQ57ss2T4L2",
        "outputId": "f1435f60-d8b1-4687-c937-2ebb36534f04"
      },
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1378\n",
            "173\n",
            "1377\n",
            "1354\n",
            "173\n",
            "173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create 4 Dimensional Array of Training Features called \"xtrain\"\n",
        "\n",
        "#xtrain is a 4d array of all the picture data that is ready to be fed into the classifiers\n",
        "xtrain = []\n",
        "for i in range(0, len(train_df)):\n",
        "  img = Image.open(train_df.filepaths[i]).convert('RGB')\n",
        "  img = np.array(img)\n",
        "  xtrain.append(img)\n",
        "xtrain = np.array(xtrain)\n",
        "print(xtrain.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7jzT1y5VKom",
        "outputId": "931707be-fbd9-46c8-ffb0-b9ca7d77d00e"
      },
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1354, 300, 300, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Array of Testing Features called \"xtest\"\n",
        "\n",
        "xtest = []\n",
        "for i in range(0, len(test_df)):\n",
        "  img = Image.open(test_df.filepaths[i]).convert('RGB')\n",
        "  img = np.array(img)\n",
        "  xtest.append(img)\n",
        "xtest = np.array(xtest)\n",
        "print(xtest.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hDUoOGVVKrP",
        "outputId": "0e71daab-57f1-41c7-867c-7c05d2b2996f"
      },
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(173, 300, 300, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create List of Training Labels called \"ytrain\"\n",
        "ytrain = []\n",
        "for i in range(0, len(train_df)):\n",
        "  label = train_df.labels[i]\n",
        "  ytrain.append(label)\n",
        "ytrain = np.array(ytrain)"
      ],
      "metadata": {
        "id": "xHtFQb-aVKvm"
      },
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create List of Testing Labels called \"ytest\"\n",
        "ytest = []\n",
        "for i in range(0, len(test_df)):\n",
        "  label = test_df.labels[i]\n",
        "  ytest.append(label)\n",
        "ytest = np.array(ytest)"
      ],
      "metadata": {
        "id": "NuZYzNvwVKxb"
      },
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting arrays of classes to numbers to encode them\n",
        "for i in range(0,len(ytrain)):\n",
        "  if ytrain[i] == \"cheetah\":\n",
        "    ytrain[i] = 0\n",
        "  if ytrain[i] == \"fox\":\n",
        "    ytrain[i] = 1\n",
        "  if ytrain[i] == \"hyena\":\n",
        "    ytrain[i] = 2\n",
        "  if ytrain[i] == \"lion\":\n",
        "    ytrain[i] = 3\n",
        "  if ytrain[i] == \"tiger\":\n",
        "    ytrain[i] = 4\n",
        "  if ytrain[i] == \"wolf\":\n",
        "    ytrain[i] = 5\n",
        "\n",
        "for i in range(0,len(ytest)):\n",
        "  if ytest[i] == \"cheetah\":\n",
        "    ytest[i] = 0\n",
        "  if ytest[i] == \"fox\":\n",
        "    ytest[i] = 1\n",
        "  if ytest[i] == \"hyena\":\n",
        "    ytest[i] = 2\n",
        "  if ytest[i] == \"lion\":\n",
        "    ytest[i] = 3\n",
        "  if ytest[i] == \"tiger\":\n",
        "    ytest[i] = 4\n",
        "  if ytest[i] == \"wolf\":\n",
        "    ytest[i] = 5\n"
      ],
      "metadata": {
        "id": "fq2Ix1U6zITD"
      },
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# kNN Classifier Run 1"
      ],
      "metadata": {
        "id": "ijndvrrxpbRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "id": "4gRGrqAzoEYV"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = KNeighborsClassifier(n_neighbors = 5)\n",
        "model.fit(train_data, classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeqAAmMJoJFy",
        "outputId": "ed1e31b0-68c6-4661-9c41-d88cbd6373ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I99yZKTcpmqX",
        "outputId": "80de3720-4fd4-4ac8-f336-8192dce49ec2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cheetah', 'fox', 'hyena', 'lion', 'tiger', 'wolf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {
        "id": "U76pJYdUGM3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow_decision_forests\n",
        "! pip install wurlitzer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3_7PUxaKSBm",
        "outputId": "ab68907d-c621-4bf8-d76c-c1708e606b74"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_decision_forests in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: tensorflow~=2.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (2.11.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (1.3.5)\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (1.21.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (1.3.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (0.38.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests) (3.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests) (22.11.23)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests) (1.50.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests) (4.1.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests) (1.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests) (21.3)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests) (2.11.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests) (0.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests) (57.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests) (2.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests) (2.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests) (14.0.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests) (0.27.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests) (3.3.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests) (2.11.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests) (0.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.11.0->tensorflow_decision_forests) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests) (2.14.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow~=2.11.0->tensorflow_decision_forests) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tensorflow_decision_forests) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tensorflow_decision_forests) (2.8.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.7/dist-packages (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Forest\n",
        "label = \"labels\" \n",
        "import tensorflow_decision_forests as tfdf\n",
        "\n",
        "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=label)\n",
        "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df, label=label)\n",
        "train_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NngHIWrGM-6",
        "outputId": "5c7a7b28-14b4-42d1-cb04-c1e3e06ce166"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=({'filepaths': TensorSpec(shape=(None,), dtype=tf.string, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label = \"labels\" \n",
        "classes = train_df[label].unique().tolist()\n",
        "print(f\"Label classes: {classes}\")\n",
        "\n",
        "#dataset_df[label] = dataset_df[label].map(classes.index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS4UXkMRLJmj",
        "outputId": "52f641e3-e719-41ce-cdcb-29d510a460cf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label classes: ['lion', 'wolf', 'tiger', 'cheetah', 'fox', 'hyena']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%set_cell_height 300\n",
        "\n",
        "# Specify the model.\n",
        "model_1 = tfdf.keras.RandomForestModel(verbose=2)\n",
        "\n",
        "# Train the model.\n",
        "model_1.fit(x=train_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZQEHABlGNOe",
        "outputId": "5f546171-cd9d-4e33-c44d-6decd076d11c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use 2 thread(s) for training\n",
            "Use /tmp/tmpkx5q4kuo as temporary training directory\n",
            "Reading training dataset...\n",
            "Training tensor examples:\n",
            "Features: {'filepaths': <tf.Tensor 'data:0' shape=(None,) dtype=string>}\n",
            "Label: Tensor(\"data_1:0\", shape=(None,), dtype=int64)\n",
            "Weights: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized tensor features:\n",
            " {'filepaths': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data:0' shape=(None,) dtype=string>)}\n",
            "Training dataset read in 0:00:05.302829. Found 1378 examples.\n",
            "Training model...\n",
            "Standard output detected as not visible to the user e.g. running in a notebook. Creating a training log redirection. If training get stuck, try calling tfdf.keras.set_training_logs_redirection(False).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO 2022-11-30T01:37:29.780845521+00:00 kernel.cc:814] Start Yggdrasil model training\n",
            "[INFO 2022-11-30T01:37:29.781055286+00:00 kernel.cc:815] Collect training examples\n",
            "[INFO 2022-11-30T01:37:29.781168798+00:00 kernel.cc:423] Number of batches: 2\n",
            "[INFO 2022-11-30T01:37:29.781182351+00:00 kernel.cc:424] Number of examples: 1378\n",
            "[INFO 2022-11-30T01:37:29.783651068+00:00 data_spec_inference.cc:303] 1378 item(s) have been pruned (i.e. they are considered out of dictionary) for the column filepaths (0 item(s) left) because min_value_count=5 and max_number_of_unique_values=2000\n",
            "[INFO 2022-11-30T01:37:29.784673724+00:00 kernel.cc:837] Training dataset:\n",
            "Number of records: 1378\n",
            "Number of columns: 2\n",
            "\n",
            "Number of columns by type:\n",
            "\tCATEGORICAL: 2 (100%)\n",
            "\n",
            "Columns:\n",
            "\n",
            "CATEGORICAL: 2 (100%)\n",
            "\t0: \"filepaths\" CATEGORICAL has-dict vocab-size:1 num-oods:1378 (100%)\n",
            "\t1: \"__LABEL\" CATEGORICAL integerized vocab-size:7 no-ood-item\n",
            "\n",
            "Terminology:\n",
            "\tnas: Number of non-available (i.e. missing) values.\n",
            "\tood: Out of dictionary.\n",
            "\tmanually-defined: Attribute which type is manually defined by the user i.e. the type was not automatically inferred.\n",
            "\ttokenized: The attribute value is obtained through tokenization.\n",
            "\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n",
            "\tvocab-size: Number of unique values.\n",
            "\n",
            "[INFO 2022-11-30T01:37:29.785385894+00:00 kernel.cc:883] Configure learner\n",
            "[INFO 2022-11-30T01:37:29.78605487+00:00 kernel.cc:913] Training config:\n",
            "learner: \"RANDOM_FOREST\"\n",
            "features: \"filepaths\"\n",
            "label: \"__LABEL\"\n",
            "task: CLASSIFICATION\n",
            "random_seed: 123456\n",
            "metadata {\n",
            "  framework: \"TF Keras\"\n",
            "}\n",
            "pure_serving_model: false\n",
            "[yggdrasil_decision_forests.model.random_forest.proto.random_forest_config] {\n",
            "  num_trees: 300\n",
            "  decision_tree {\n",
            "    max_depth: 16\n",
            "    min_examples: 5\n",
            "    in_split_min_examples_check: true\n",
            "    keep_non_leaf_label_distribution: true\n",
            "    num_candidate_attributes: 0\n",
            "    missing_value_policy: GLOBAL_IMPUTATION\n",
            "    allow_na_conditions: false\n",
            "    categorical_set_greedy_forward {\n",
            "      sampling: 0.1\n",
            "      max_num_items: -1\n",
            "      min_item_frequency: 1\n",
            "    }\n",
            "    growing_strategy_local {\n",
            "    }\n",
            "    categorical {\n",
            "      cart {\n",
            "      }\n",
            "    }\n",
            "    axis_aligned_split {\n",
            "    }\n",
            "    internal {\n",
            "      sorting_strategy: PRESORTED\n",
            "    }\n",
            "    uplift {\n",
            "      min_examples_in_treatment: 5\n",
            "      split_score: KULLBACK_LEIBLER\n",
            "    }\n",
            "  }\n",
            "  winner_take_all_inference: true\n",
            "  compute_oob_performances: true\n",
            "  compute_oob_variable_importances: false\n",
            "  num_oob_variable_importances_permutations: 1\n",
            "  bootstrap_training_dataset: true\n",
            "  bootstrap_size_ratio: 1\n",
            "  adapt_bootstrap_size_ratio_for_maximum_training_duration: false\n",
            "  sampling_with_replacement: true\n",
            "}\n",
            "\n",
            "[INFO 2022-11-30T01:37:29.790716734+00:00 kernel.cc:916] Deployment config:\n",
            "cache_path: \"/tmp/tmpkx5q4kuo/working_cache\"\n",
            "num_threads: 2\n",
            "try_resume_training: true\n",
            "\n",
            "[INFO 2022-11-30T01:37:29.790773536+00:00 kernel.cc:945] Train model\n",
            "[INFO 2022-11-30T01:37:29.790977459+00:00 random_forest.cc:407] Training random forest on 1378 example(s) and 1 feature(s).\n",
            "[INFO 2022-11-30T01:37:29.795773216+00:00 random_forest.cc:796] Training of tree  1/300 (tree index:0) done accuracy:0.183953 logloss:29.4133\n",
            "[INFO 2022-11-30T01:37:29.798065376+00:00 random_forest.cc:796] Training of tree  11/300 (tree index:10) done accuracy:0.196337 logloss:26.707\n",
            "[INFO 2022-11-30T01:37:29.80642402+00:00 random_forest.cc:796] Training of tree  21/300 (tree index:21) done accuracy:0.198983 logloss:22.6835\n",
            "[INFO 2022-11-30T01:37:29.811370168+00:00 random_forest.cc:796] Training of tree  31/300 (tree index:31) done accuracy:0.198839 logloss:22.1374\n",
            "[INFO 2022-11-30T01:37:29.816299265+00:00 random_forest.cc:796] Training of tree  41/300 (tree index:41) done accuracy:0.198839 logloss:21.6545\n",
            "[INFO 2022-11-30T01:37:29.818295412+00:00 random_forest.cc:796] Training of tree  51/300 (tree index:50) done accuracy:0.198839 logloss:21.4288\n",
            "[INFO 2022-11-30T01:37:29.819599325+00:00 random_forest.cc:796] Training of tree  61/300 (tree index:60) done accuracy:0.198839 logloss:19.411\n",
            "[INFO 2022-11-30T01:37:29.821373665+00:00 random_forest.cc:796] Training of tree  71/300 (tree index:70) done accuracy:0.198839 logloss:19.3354\n",
            "[INFO 2022-11-30T01:37:29.827609043+00:00 random_forest.cc:796] Training of tree  81/300 (tree index:81) done accuracy:0.198839 logloss:19.2361\n",
            "[INFO 2022-11-30T01:37:29.831566358+00:00 random_forest.cc:796] Training of tree  91/300 (tree index:91) done accuracy:0.198839 logloss:19.245\n",
            "[INFO 2022-11-30T01:37:29.839308763+00:00 random_forest.cc:796] Training of tree  101/300 (tree index:101) done accuracy:0.198839 logloss:19.2726\n",
            "[INFO 2022-11-30T01:37:29.844513317+00:00 random_forest.cc:796] Training of tree  111/300 (tree index:93) done accuracy:0.198839 logloss:19.2566\n",
            "[INFO 2022-11-30T01:37:29.849687893+00:00 random_forest.cc:796] Training of tree  121/300 (tree index:121) done accuracy:0.198839 logloss:18.5996\n",
            "[INFO 2022-11-30T01:37:29.851657403+00:00 random_forest.cc:796] Training of tree  131/300 (tree index:130) done accuracy:0.198839 logloss:18.5898\n",
            "[INFO 2022-11-30T01:37:29.852967541+00:00 random_forest.cc:796] Training of tree  141/300 (tree index:140) done accuracy:0.198839 logloss:18.59\n",
            "[INFO 2022-11-30T01:37:29.854566653+00:00 random_forest.cc:796] Training of tree  151/300 (tree index:150) done accuracy:0.198839 logloss:18.6002\n",
            "[INFO 2022-11-30T01:37:29.856246356+00:00 random_forest.cc:796] Training of tree  161/300 (tree index:160) done accuracy:0.198839 logloss:18.2103\n",
            "[INFO 2022-11-30T01:37:29.863967576+00:00 random_forest.cc:796] Training of tree  171/300 (tree index:171) done accuracy:0.198839 logloss:17.7697\n",
            "[INFO 2022-11-30T01:37:29.866556445+00:00 random_forest.cc:796] Training of tree  181/300 (tree index:181) done accuracy:0.198839 logloss:17.7862\n",
            "[INFO 2022-11-30T01:37:29.874334109+00:00 random_forest.cc:796] Training of tree  191/300 (tree index:191) done accuracy:0.198839 logloss:17.8026\n",
            "[INFO 2022-11-30T01:37:29.878234619+00:00 random_forest.cc:796] Training of tree  201/300 (tree index:183) done accuracy:0.198839 logloss:17.8095\n",
            "[INFO 2022-11-30T01:37:29.8806637+00:00 random_forest.cc:796] Training of tree  211/300 (tree index:210) done accuracy:0.198839 logloss:17.823\n",
            "[INFO 2022-11-30T01:37:29.882638178+00:00 random_forest.cc:796] Training of tree  221/300 (tree index:220) done accuracy:0.198839 logloss:17.8377\n",
            "[INFO 2022-11-30T01:37:29.88418989+00:00 random_forest.cc:796] Training of tree  231/300 (tree index:230) done accuracy:0.198839 logloss:17.7253\n",
            "[INFO 2022-11-30T01:37:29.88553324+00:00 random_forest.cc:796] Training of tree  241/300 (tree index:240) done accuracy:0.198839 logloss:17.7384\n",
            "[INFO 2022-11-30T01:37:29.889356703+00:00 random_forest.cc:796] Training of tree  251/300 (tree index:251) done accuracy:0.198839 logloss:17.7436\n",
            "[INFO 2022-11-30T01:37:29.891713374+00:00 random_forest.cc:796] Training of tree  261/300 (tree index:261) done accuracy:0.198839 logloss:17.7483\n",
            "[INFO 2022-11-30T01:37:29.893244072+00:00 random_forest.cc:796] Training of tree  271/300 (tree index:270) done accuracy:0.198839 logloss:17.76\n",
            "[INFO 2022-11-30T01:37:29.896771085+00:00 random_forest.cc:796] Training of tree  281/300 (tree index:277) done accuracy:0.198839 logloss:17.765\n",
            "[INFO 2022-11-30T01:37:29.900644642+00:00 random_forest.cc:796] Training of tree  291/300 (tree index:286) done accuracy:0.198839 logloss:17.6412\n",
            "[INFO 2022-11-30T01:37:29.907834206+00:00 random_forest.cc:796] Training of tree  300/300 (tree index:295) done accuracy:0.198839 logloss:17.5876\n",
            "[INFO 2022-11-30T01:37:29.908004484+00:00 random_forest.cc:876] Final OOB metrics: accuracy:0.198839 logloss:17.5876\n",
            "[INFO 2022-11-30T01:37:29.908263888+00:00 kernel.cc:962] Export model in log directory: /tmp/tmpkx5q4kuo with prefix 414e807ac2e64c8d\n",
            "[INFO 2022-11-30T01:37:29.910588398+00:00 kernel.cc:979] Save model in resources\n",
            "[INFO 2022-11-30T01:37:29.922192002+00:00 abstract_model.cc:844] Model self evaluation:\n",
            "Number of predictions (without weights): 1378\n",
            "Number of predictions (with weights): 1378\n",
            "Task: CLASSIFICATION\n",
            "Label: __LABEL\n",
            "\n",
            "Accuracy: 0.198839  CI95[W][0.181278 0.217357]\n",
            "LogLoss: : 17.5876\n",
            "ErrorRate: : 0.801161\n",
            "\n",
            "Default Accuracy: : 0.198839\n",
            "Default LogLoss: : 1.78604\n",
            "Default ErrorRate: : 0.801161\n",
            "\n",
            "Confusion Table:\n",
            "truth\\prediction\n",
            "   0    1  2  3  4  5  6\n",
            "0  0    0  0  0  0  0  0\n",
            "1  0  274  0  0  0  0  0\n",
            "2  0  200  0  0  0  0  0\n",
            "3  0  244  0  0  0  0  0\n",
            "4  0  235  0  0  0  0  0\n",
            "5  0  215  0  0  0  0  0\n",
            "6  0  210  0  0  0  0  0\n",
            "Total: 1378\n",
            "\n",
            "One vs other classes:\n",
            "\n",
            "[INFO 2022-11-30T01:37:29.947495256+00:00 kernel.cc:1175] Loading model from path /tmp/tmpkx5q4kuo/model/ with prefix 414e807ac2e64c8d\n",
            "[WARNING 2022-11-30T01:37:29.949507464+00:00 utils.cc:73] The model does not have any input features i.e. the model is constant and will always return the same prediction.\n",
            "[INFO 2022-11-30T01:37:29.949616635+00:00 decision_forest.cc:640] Model loaded with 300 root(s), 300 node(s), and 0 input feature(s).\n",
            "[INFO 2022-11-30T01:37:29.949641771+00:00 abstract_model.cc:1306] Engine \"RandomForestGeneric\" built\n",
            "[INFO 2022-11-30T01:37:29.949674064+00:00 kernel.cc:1021] Use fast generic engine\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained in 0:00:00.206287\n",
            "Compiling model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7ff080932710> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7ff080932710> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "Model compiled.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff06993bdd0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.compile(metrics=[\"accuracy\"])\n",
        "evaluation = model_1.evaluate(test_ds, return_dict=True)\n",
        "print()\n",
        "\n",
        "for name, value in evaluation.items():\n",
        "  print(f\"{name}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw8ENVA-GOsN",
        "outputId": "1c1359c0-821d-4375-a5d0-ed7c86ad54e0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 408ms/step - loss: 0.0000e+00 - accuracy: 0.1965\n",
            "\n",
            "loss: 0.0000\n",
            "accuracy: 0.1965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P25RKKRkkW60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzbWN6MeGOuj",
        "outputId": "849b2221-c861-4a6f-919e-50c7b60c2462"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"random_forest_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            "=================================================================\n",
            "Total params: 1\n",
            "Trainable params: 0\n",
            "Non-trainable params: 1\n",
            "_________________________________________________________________\n",
            "Type: \"RANDOM_FOREST\"\n",
            "Task: CLASSIFICATION\n",
            "Label: \"__LABEL\"\n",
            "\n",
            "Input Features (1):\n",
            "\tfilepaths\n",
            "\n",
            "No weights\n",
            "\n",
            "Variable Importance: MEAN_MIN_DEPTH:\n",
            "    1. \"filepaths\"  0.000000 \n",
            "    2.   \"__LABEL\"  0.000000 \n",
            "\n",
            "Variable Importance: NUM_AS_ROOT:\n",
            "\n",
            "Variable Importance: NUM_NODES:\n",
            "\n",
            "Variable Importance: SUM_SCORE:\n",
            "\n",
            "\n",
            "\n",
            "Winner takes all: true\n",
            "Out-of-bag evaluation: accuracy:0.198839 logloss:17.5876\n",
            "Number of trees: 300\n",
            "Total number of nodes: 300\n",
            "\n",
            "Number of nodes by tree:\n",
            "Count: 300 Average: 1 StdDev: 0\n",
            "Min: 1 Max: 1 Ignored: 0\n",
            "----------------------------------------------\n",
            "[ 1, 1] 300 100.00% 100.00% ##########\n",
            "\n",
            "Depth by leafs:\n",
            "Count: 300 Average: 0 StdDev: 0\n",
            "Min: 0 Max: 0 Ignored: 0\n",
            "----------------------------------------------\n",
            "[ 0, 0] 300 100.00% 100.00% ##########\n",
            "\n",
            "Number of training obs by leaf:\n",
            "Count: 300 Average: 1378 StdDev: 0\n",
            "Min: 1378 Max: 1378 Ignored: 0\n",
            "----------------------------------------------\n",
            "[ 1378, 1378] 300 100.00% 100.00% ##########\n",
            "\n",
            "Attribute in nodes:\n",
            "\n",
            "Attribute in nodes with depth <= 0:\n",
            "\n",
            "Attribute in nodes with depth <= 1:\n",
            "\n",
            "Attribute in nodes with depth <= 2:\n",
            "\n",
            "Attribute in nodes with depth <= 3:\n",
            "\n",
            "Attribute in nodes with depth <= 5:\n",
            "\n",
            "Condition type in nodes:\n",
            "Condition type in nodes with depth <= 0:\n",
            "Condition type in nodes with depth <= 1:\n",
            "Condition type in nodes with depth <= 2:\n",
            "Condition type in nodes with depth <= 3:\n",
            "Condition type in nodes with depth <= 5:\n",
            "Node format: NOT_SET\n",
            "\n",
            "Training OOB:\n",
            "\ttrees: 1, Out-of-bag evaluation: accuracy:0.183953 logloss:29.4133\n",
            "\ttrees: 11, Out-of-bag evaluation: accuracy:0.196337 logloss:26.707\n",
            "\ttrees: 21, Out-of-bag evaluation: accuracy:0.198983 logloss:22.6835\n",
            "\ttrees: 31, Out-of-bag evaluation: accuracy:0.198839 logloss:22.1374\n",
            "\ttrees: 41, Out-of-bag evaluation: accuracy:0.198839 logloss:21.6545\n",
            "\ttrees: 51, Out-of-bag evaluation: accuracy:0.198839 logloss:21.4288\n",
            "\ttrees: 61, Out-of-bag evaluation: accuracy:0.198839 logloss:19.411\n",
            "\ttrees: 71, Out-of-bag evaluation: accuracy:0.198839 logloss:19.3354\n",
            "\ttrees: 81, Out-of-bag evaluation: accuracy:0.198839 logloss:19.2361\n",
            "\ttrees: 91, Out-of-bag evaluation: accuracy:0.198839 logloss:19.245\n",
            "\ttrees: 101, Out-of-bag evaluation: accuracy:0.198839 logloss:19.2726\n",
            "\ttrees: 111, Out-of-bag evaluation: accuracy:0.198839 logloss:19.2566\n",
            "\ttrees: 121, Out-of-bag evaluation: accuracy:0.198839 logloss:18.5996\n",
            "\ttrees: 131, Out-of-bag evaluation: accuracy:0.198839 logloss:18.5898\n",
            "\ttrees: 141, Out-of-bag evaluation: accuracy:0.198839 logloss:18.59\n",
            "\ttrees: 151, Out-of-bag evaluation: accuracy:0.198839 logloss:18.6002\n",
            "\ttrees: 161, Out-of-bag evaluation: accuracy:0.198839 logloss:18.2103\n",
            "\ttrees: 171, Out-of-bag evaluation: accuracy:0.198839 logloss:17.7697\n",
            "\ttrees: 181, Out-of-bag evaluation: accuracy:0.198839 logloss:17.7862\n",
            "\ttrees: 191, Out-of-bag evaluation: accuracy:0.198839 logloss:17.8026\n",
            "\ttrees: 201, Out-of-bag evaluation: accuracy:0.198839 logloss:17.8095\n",
            "\ttrees: 211, Out-of-bag evaluation: accuracy:0.198839 logloss:17.823\n",
            "\ttrees: 221, Out-of-bag evaluation: accuracy:0.198839 logloss:17.8377\n",
            "\ttrees: 231, Out-of-bag evaluation: accuracy:0.198839 logloss:17.7253\n",
            "\ttrees: 241, Out-of-bag evaluation: accuracy:0.198839 logloss:17.7384\n",
            "\ttrees: 251, Out-of-bag evaluation: accuracy:0.198839 logloss:17.7436\n",
            "\ttrees: 261, Out-of-bag evaluation: accuracy:0.198839 logloss:17.7483\n",
            "\ttrees: 271, Out-of-bag evaluation: accuracy:0.198839 logloss:17.76\n",
            "\ttrees: 281, Out-of-bag evaluation: accuracy:0.198839 logloss:17.765\n",
            "\ttrees: 291, Out-of-bag evaluation: accuracy:0.198839 logloss:17.6412\n",
            "\ttrees: 300, Out-of-bag evaluation: accuracy:0.198839 logloss:17.5876\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Run 1"
      ],
      "metadata": {
        "id": "yK83-Nc5MaKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\"\"\"\n",
        "The code in this cell and the next 2 cells is loosely based on the code in the following tutorial: \n",
        "https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/Course%202%20-%20Part%208%20-%20Lesson%202%20-%20Notebook%20(RockPaperScissors).ipynb#scrollTo=ZABJp7T3VLCU\n",
        "I say loosely based because while I learned the essence of how to set up, compile and fit the model from this tutorial it would not have worked for the problem I am trying to solve,\n",
        "I had to learn what all the code, parameters and hyperparamters meant from documentation and tutorials (that I've linked in my blog post) and then write code that would actually work for \n",
        "my particular problem. Therefore these next few cells are where most of my unique contributions are.\n",
        "\"\"\"\n",
        "#Rescaling images\n",
        "train_datagen = ImageDataGenerator(rescale= 1.0/255)\n",
        "test_datagen = ImageDataGenerator(rescale= 1.0/255)\n",
        "valid_datagen = ImageDataGenerator(rescale= 1.0/255) \n",
        "\n",
        "#Generating batches of images for training, validation, and testing along with their labels to be fed into the neural net\n",
        "#Read documentation on .flow_from_dataframe() from tensorflow website: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_dataframe\n",
        "train_data = train_datagen.flow_from_dataframe(train_df, x_col = \"filepaths\", y_col = \"labels\", batch_size =50, class_mode= \"categorical\", target_size = (224,224))\n",
        "test_data = test_datagen.flow_from_dataframe(test_df, x_col = \"filepaths\", y_col = \"labels\", batch_size =50, class_mode= \"categorical\", target_size = (224,224))\n",
        "valid_data = valid_datagen.flow_from_dataframe(valid_df, x_col = \"filepaths\", y_col = \"labels\", batch_size =50, class_mode= \"categorical\", target_size = (224,224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMYiGM6ngEpc",
        "outputId": "298463b9-f9a0-4380-bfb7-bbf6786e34d5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1378 validated image filenames belonging to 6 classes.\n",
            "Found 173 validated image filenames belonging to 6 classes.\n",
            "Found 172 validated image filenames belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting images to greyscale\n",
        "#Create 4 Dimensional Array of Training Features called \"xtrain\"\n",
        "\n",
        "#xtrain is a 4d array of all the picture data that is ready to be fed into the classifiers\n",
        "xtrain = []\n",
        "for i in range(0, len(train_df)):\n",
        "  img = Image.open(train_df.filepaths[i]).convert('L')\n",
        "  img = np.array(img)\n",
        "  xtrain.append(img)\n",
        "xtrain = np.array(xtrain)\n",
        "print(xtrain.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55ubha4uY6EX",
        "outputId": "06bc2bae-7de2-460e-eb45-fece688da479"
      },
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1354, 300, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Array of Testing Features called \"xtest\"\n",
        "\n",
        "xtest = []\n",
        "for i in range(0, len(test_df)):\n",
        "  img = Image.open(test_df.filepaths[i]).convert('L')\n",
        "  img = np.array(img)\n",
        "  xtest.append(img)\n",
        "xtest = np.array(xtest)\n",
        "print(xtest.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi_0242FY6Gy",
        "outputId": "6a2020d5-07ff-465f-b44f-f6e24f40feac"
      },
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(173, 300, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Note:the code in the next two blocks is heavily based on https://www.tensorflow.org/guide/keras/rnn.\n",
        "     My original contribution is in trying to optimize the RNN by doing things such as adding different layers and trying different activation functions. \n",
        "'''\n",
        "batch_size =50\n",
        "input_dim = 300"
      ],
      "metadata": {
        "id": "KmDjBd9J3KJH"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential()\n",
        "# Add an Embedding layer expecting input vocab of size 1000, and\n",
        "# output embedding dimension of size 64.\n",
        "#model.add(layers.Embedding(input_dim, output_dim=64))\n",
        "\n",
        "\n",
        "#Add a LSTM layer with 128 internal units.\n",
        "model.add(layers.LSTM(128, input_shape = (300,300) ,activation = 'relu', return_sequences = False))\n",
        "\n",
        "# Add a Dense layer with 6 units.\n",
        "model.add(layers.Dense(6))\n",
        "\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt-Qg-Y3MZ4H",
        "outputId": "ca0745ec-7394-4c5e-db8c-48e117ee0139"
      },
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_19 (LSTM)              (None, 128)               219648    \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 220,422\n",
            "Trainable params: 220,422\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = \"mean_squared_error\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"],)\n",
        "\n",
        "history = model.fit(xtrain, ytrain, epochs = 3, validation_data = (xtest, ytest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uTVv0Aun3VTN",
        "outputId": "1c81923e-8c36-4d2d-d476-16e3344e022d"
      },
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnimplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-292-8184044525c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m               metrics=[\"accuracy\"],)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'mean_squared_error/Cast' defined at (most recent call last):\n    File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 149, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 690, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/ioloop.py\", line 743, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 787, in inner\n      self.run()\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 748, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n      user_expressions, allow_stdin,\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2855, in run_cell\n      raw_cell, store_history, silent, shell_futures)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3058, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-292-8184044525c4>\", line 5, in <module>\n      history = model.fit(xtrain, ytrain, epochs = 3, validation_data = (xtest, ytest))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 890, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 949, in compute_loss\n      y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 139, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 243, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1326, in mean_squared_error\n      y_true = tf.cast(y_true, y_pred.dtype)\nNode: 'mean_squared_error/Cast'\nCast string to float is not supported\n\t [[{{node mean_squared_error/Cast}}]] [Op:__inference_train_function_19009]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Run 2"
      ],
      "metadata": {
        "id": "4IkJiCN14zvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.Input(shape = (1378,300)))\n",
        "model.add(layers.SimpleRNN(128, activation = 'relu'))\n",
        "model.add(layers.Dense(6))\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"],)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4T77BDPkdGc",
        "outputId": "0650233e-4785-45a0-b2cc-0c26c4efed70"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_3 (SimpleRNN)    (None, 128)               54912     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 55,686\n",
            "Trainable params: 55,686\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_data, batch_size = batch_size, epochs = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "S4ZDoOkufgPQ",
        "outputId": "6e9468c1-df98-4863-c7ad-28c22d3332db"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 1378, 300) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1378, 300), dtype=tf.float32, name='input_4'), name='input_4', description=\"created by layer 'input_4'\"), but it was called on an input with incompatible shape (None, None, None, None).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-85ad84ba1b29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 214, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential_6\" (type Sequential).\n    \n    Input 0 of layer \"simple_rnn_3\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, None, None, None)\n    \n    Call arguments received by layer \"sequential_6\" (type Sequential):\n      • inputs=tf.Tensor(shape=(None, None, None, None), dtype=float32)\n      • training=True\n      • mask=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM Classifier Run 1"
      ],
      "metadata": {
        "id": "UI8cHiAJrLyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reshaping training data to pass into SVM classifier\n",
        "svm_xtrain = xtrain.reshape(1354, 90000)\n",
        "svm_xtrain.shape\n",
        "\n",
        "#Reshaping testing data to pass into SVM classifier\n",
        "svm_xtest = xtest.reshape(173, 90000)\n",
        "svm_xtest.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZZ2TC-a2uis",
        "outputId": "4100ea5a-93cb-4d2c-c194-d8d3fe6686ae"
      },
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(173, 90000)"
            ]
          },
          "metadata": {},
          "execution_count": 298
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the SVM classifier model\n",
        "\n",
        "classifier = svm.SVC(kernel = \"linear\", gamma = \"auto\", C = 2)\n",
        "classifier.fit(svm_xtrain, ytrain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dYxNtvPJESOV",
        "outputId": "bdb5bffa-9721-4eca-97be-44745e6396d8"
      },
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprediction = classifier.predict(xtest)\\nacc = classifier.score(xtest, ytest)\\nprint(\"Accuracy: \", acc)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = classifier.score(svm_xtest, ytest)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZPWGHzd4pMi",
        "outputId": "f7ea10d3-d098-4a49-9efb-58333faef839"
      },
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2543352601156069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#SVM Classifier Run 2\n",
        "\n"
      ],
      "metadata": {
        "id": "AwLj6_SYb-D9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the SVM classifier model\n",
        "\n",
        "classifier = svm.SVC(kernel = \"poly\", gamma = \"auto\", C = 2)\n",
        "classifier.fit(svm_xtrain, ytrain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDE_mWGt6GWT",
        "outputId": "333eff00-f400-4f70-91eb-04d5c3eb5949"
      },
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=2, gamma='auto', kernel='poly')"
            ]
          },
          "metadata": {},
          "execution_count": 303
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = classifier.score(svm_xtest, ytest)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-fgSAm6OHA",
        "outputId": "e99c8854-26cd-4e57-ebe8-b53b93e7ff93"
      },
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3063583815028902\n"
          ]
        }
      ]
    }
  ]
}